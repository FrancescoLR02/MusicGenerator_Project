{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f628c087",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m#neural network module\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;66;03m#optimizer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn #neural network module\n",
    "import torch.optim as optim #optimizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90129f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROVA SCHEMA\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, instrument_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.instrument_size = instrument_size\n",
    "\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        #reshape to (batch size,256,1,2)\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, x, kernel_size=(1,2), stride=2, bias=False, padding=2),\n",
    "            nn.BatchNorm2d(x),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(x, y, kernel_size=(1,2), stride=2, bias=False, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(y),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(y, z, kernel_size=(1,2), stride=2, bias=False, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(z),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(z, instrument_size, kernel_size=(128,1), stride=1, bias=False, padding=2, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y= self.fc_net(x)\n",
    "        y = y.reshape((-1, 256, 1, 2))\n",
    "        y = self.conv_model(y)\n",
    "        return y\n",
    "    \n",
    "    class Conditioner(nn.Module):\n",
    "\n",
    "    def __init__(self,instrument_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.instrument_size = instrument_size\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv2d(instrument_size, z, kernel_size=(128,1), stride=1, bias=False, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(z, y, kernel_size=(1,2), stride=2, bias=False, padding=2),\n",
    "            #nn.BatchNorm2d(z),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(y, x, kernel_size=(1,2), stride=2, bias=False, padding=2),\n",
    "            #nn.BatchNorm2d(y),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(x, 256, kernel_size=(1,2), stride=2, bias=False, padding=2),\n",
    "            #nn.BatchNorm2d(x),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv_model(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_cond_concat(x, y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "    x_shapes = x.shape  #[batch,n_features,a,b]\n",
    "    y_shapes = y.shape  #[batch,n]\n",
    "    y2 = y.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])      #[batch,n,a,b]\n",
    "\n",
    "    return torch.cat((x, y2),dim=1) #[batch,n_features+n,a,b]\n",
    "\n",
    "def conv_prev_concat(x, y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "    x_shapes = x.shape  #[batch,n_features,a,b]\n",
    "    y_shapes = y.shape  #[batch,16,a,b]\n",
    "    if x_shapes[2:] == y_shapes[2:]:\n",
    "        y2 = y.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])  #[batch,16,a,b]\n",
    "\n",
    "        return torch.cat((x, y2),dim=1) #[batch,n_features+16,a,b]\n",
    "\n",
    "    else:\n",
    "        print(x_shapes[2:])\n",
    "        print(y_shapes[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88728a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, cond_1d_size, instrument_size=1, n_hlayers=128):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.input_size = input_size\n",
    "            self.instrument_size = instrument_size\n",
    "            self.cond1d_dim = cond_1d_size\n",
    "            \n",
    "            #generator layers\n",
    "            #as said in the DCGAN paper always ReLU activation function in the generator excluded the last layer\n",
    "            #as said in the DCGAN paper always batchnorm iin the generator excluded the last layer\n",
    "            self.ff1 = nn.Sequential(\n",
    "                nn.Linear(input_size+cond_1d_size, 1024),                                                               #[batch,1024]\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU()       \n",
    "                )\n",
    "            self.ff2 = nn.Sequential(\n",
    "                nn.Linear(1024+cond_1d_size,n_hlayers*2),                                                                                    #[batch,512]\n",
    "                nn.BatchNorm1d(n_hlayers*2),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            #reshape to [batch size,128,1,2]\n",
    "            # #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "            self.cc1 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, n_hlayers, kernel_size=(1,2), stride=(2,2), bias=False, padding=0),           #[batch,128,1,4]\n",
    "                nn.BatchNorm2d(n_hlayers),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "            self.cc2 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, n_hlayers, kernel_size=(1,2), stride=(2,2), bias=False, padding=0),           #[batch,128,1,8]\n",
    "                nn.BatchNorm2d(n_hlayers),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "            self.cc3 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, n_hlayers, kernel_size=(1,2), stride=(2,2), bias=False, padding=0),           #[batch,128,1,16]\n",
    "                nn.BatchNorm2d(n_hlayers),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "            #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "            self.cc4 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, instrument_size, kernel_size=(128,1), stride=(2,1), bias=False, padding=0),       #[batch,instrument_size,128,16]\n",
    "                nn.Sigmoid()    \n",
    "                #Sigmoid funciotn because we want to generate the matrixes of music without velocity, i.e. only (0,1)\n",
    "                #Thus we use the sigmoid which is a smoother version of the sign function\n",
    "                )\n",
    "            #conditioner layers\n",
    "            # #as in Midinet model we use the Leaky activation funciton for the conditioner\n",
    "            self.h0_prev = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=instrument_size, out_channels=16, kernel_size=(128,1), stride=(2,1)),                  #[batch,16,1,16]\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU()          #note: in the original paper leak=0.2, default leak=0.01\n",
    "                )\n",
    "            self.h1_prev = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,2), stride=(2,2)),                                  #[batch,16,1,8]\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "            self.h2_prev = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,2), stride=(2,2)),                                  #[batch,16,1,4]\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "            self.h3_prev = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,2), stride=(2,2)),                                  #[batch,16,1,2]\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU()\n",
    "                )\n",
    "            \n",
    "    def forward(self, z, prev_bar, cond_1d, batch_size):\n",
    "            \n",
    "            #2d condiiton\n",
    "            cond0 = self.h0_prev(prev_bar)          #[batch,16,1,16]\n",
    "            cond1 = self.h1_prev(cond0)             #[batch,16,1,8]\n",
    "            cond2 = self.h2_prev(cond1)             #[batch,16,1,4]\n",
    "            cond3 = self.h3_prev(cond2)             #[batch,16,1,2]\n",
    "            \n",
    "            #single cond_1d size =[n,1], batch_cond_1d size = [batch_size,n]\n",
    "            \n",
    "            input = torch.cat((z,cond_1d), dim=1)   #[batch_size, input_size+cond_1d_size]\n",
    "            \n",
    "            h0 = self.ff1(input)                    #[batch,1024]\n",
    "            h0 = torch.cat((h0,cond_1d), dim=1)     #[batch,1024+cond_1d_size]\n",
    "            \n",
    "            h1 = self.ff2(h0)                       #[batch,256]\n",
    "            h1 = h1.reshape(batch_size, 128, 1, 2)  #[batch,128,1,2]\n",
    "            h1 = conv_cond_concat(h1,cond_1d)       #[batch,128+cond_1d_size,1,2]\n",
    "            h1 = conv_prev_concat(h1,cond3)         #[batch,128+cond_1d_size+16,1,2]\n",
    "            \n",
    "            h2 = self.cnn1(h1)                      #[batch,128,1,4]\n",
    "            h2 = conv_cond_concat(h2,cond_1d)       #[batch,128+cond_1d_size,1,4]\n",
    "            h2 = conv_prev_concat(h2,cond2)         #[batch,128+cond_1d_size+16,1,4]\n",
    "            \n",
    "            h3 = self.cnn2(h2)                      #[batch,128,1,8]\n",
    "            h3 = conv_cond_concat(h3,cond_1d)       #[batch,128+cond_1d_size,1,8]\n",
    "            h3 = conv_prev_concat(h3,cond1)         #[batch,128+cond_1d_size+16,1,8]\n",
    "            \n",
    "            h4 = self.cnn3(h3)                      #[batch,128,1,16]\n",
    "            h4 = conv_cond_concat(h4,cond_1d)       #[batch,128+cond_1d_size,1,16]\n",
    "            h4 = conv_prev_concat(h4,cond0)         #[batch,128+cond_1d_size+16,1,16]\n",
    "            \n",
    "            out = self.cnn4(h4)                     #[batch,instrument_size,128,16]\n",
    "            \n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, cond_1d_size, instrument_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.instrument_size = instrument_size\n",
    "        self.cond1d_dim = cond_1d_size\n",
    "        \n",
    "        #as said in the DCGAN paper always batchnorm in the discriminator layers excluded the first layer\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(2*instrument_size+cond_1d_size, 14, kernel_size=(128,2), stride=(2,2), padding=0),        #[batch,14,1,8]\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)  \n",
    "        )\n",
    "        #+condition [batch,14+cond_1d_size,1,8]\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(14+cond_1d_size, 77, kernel_size=(1,4), stride=2, padding=0),                             #[batch,77,1,3]\n",
    "            nn.BatchNorm2d(77),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.ffnn1 = nn.Sequential(\n",
    "            nn.Linear(231, 1024),\n",
    "            #+condition [batch,231+cond_1d_size]    \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        #+condition [batch,1024+cond_1d_size]\n",
    "        self.ffnn2 = nn.Linear(1024+cond_1d_size, 1)      #no sigmoid activation function because it is already in the definition of the cross entropy loss function\n",
    "\n",
    "        \n",
    "    def forward(self, x, prev_bar, cond_1d):\n",
    "        input = conv_cond_concat(x,cond_1d)         #[batch,instrument_size+cond_1d_size,128,16]\n",
    "        input = conv_prev_concat(input,prev_bar)    #[batch,2*instrument_size+cond_1d_size,128,16]\n",
    "        \n",
    "        h0 = self.cnn1(input)                       #[batch,14,1,8]\n",
    "        fm = h0\n",
    "        h0 = conv_cond_concat(h0, cond_1d)          #[batch,14+cond_1d_size,1,8]\n",
    "        \n",
    "        h1 = self.cnn2(h0)                          #[batch,77,1,3]\n",
    "        h1 = nn.Flatten(h1)                         #[batch,77*3*1]\n",
    "        h1 = torch.cat((h1,cond_1d),dim=1)          #[batch,231+cond_1d_size]\n",
    "\n",
    "        h2 = self.ffnn1(h1)                         #[batch,1024]\n",
    "        h2 = torch.cat((h2,cond_1d),dim=1)          #[batch,1024+cond_1d_size]\n",
    "\n",
    "        h3 = self.ffnn2(h2)                         #[batch,1]\n",
    "        h3_sigmoid = nn.Sigmoid(h3)\n",
    "\n",
    "\n",
    "        return h3_sigmoid, h3, fm\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.2)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e61f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(100,1)\n",
    "generator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(1)\n",
    "discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, device, alpha=0.1):\n",
    "    \n",
    "    #real_targets = torch.ones_like(real_output, device=device)                 #no label smoothing -> True output expected output is 1\n",
    "    real_targets = torch.full_like(real_output, 1.0 - alpha, device=device)     #one side label smoothing to penalize self confidence\n",
    "    fake_targets = torch.zeros_like(fake_output, device=device)                 #no label smoothing -> Fake output expected output is 0\n",
    "        \n",
    "    real_loss = cross_entropy(real_output, real_targets)\n",
    "    fake_loss = cross_entropy(fake_output, fake_targets)\n",
    "\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367712ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, device):\n",
    "    \n",
    "    gen_loss = cross_entropy(fake_output, torch.ones_like(fake_output, device=device))          #for the generator we use another value function in order to avoid vanishing gradients\n",
    "    \n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = torch.optim.Adam(generator.parameters(), lr=2e-4)\n",
    "dis_opt = torch.optim.Adam(discriminator.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b71e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
