{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ccc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import librosa.display\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Preprocessing import *\n",
    "from ExtractGenre import *\n",
    "\n",
    "import DatasetLoader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5b0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputPath = os.path.realpath('YAMF/genres_original')\n",
    "\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf63f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeSpectrogram(X):\n",
    "   mean = np.mean(X)\n",
    "   std = np.std(X)\n",
    "   return (X - mean) / (std + 1e-6)\n",
    "\n",
    "\n",
    "def DataCNN(InputPath = os.path.realpath('YAMF/genres_original'), length = 256):\n",
    "\n",
    "   numErr = 0\n",
    "\n",
    "   TrainDataList, ValDataList, DataList = [], [], []\n",
    "   for dir in tqdm(os.listdir(InputPath)):\n",
    "      \n",
    "      DirPath = os.path.join(InputPath, dir)\n",
    "\n",
    "      if not os.path.isdir(DirPath):\n",
    "         continue\n",
    "\n",
    "      genre = GenreMapping[dir]\n",
    "\n",
    "      trainSong = 0\n",
    "      for song in os.listdir(DirPath):\n",
    "         warnings.filterwarnings('ignore')\n",
    "\n",
    "         trainSong += 1\n",
    "         SongPath = os.path.join(DirPath, song)\n",
    "\n",
    "         #Train data\n",
    "         if trainSong <= 80:\n",
    "            try:\n",
    "               y, sr = librosa.load(SongPath, sr=16000)\n",
    "               mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "               S_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "            except:\n",
    "               numErr += 1\n",
    "               continue \n",
    "\n",
    "            for _ in range(20):\n",
    "\n",
    "               rIDX = np.random.randint(0, np.shape(S_db)[1] - length)\n",
    "               indexs = np.arange(rIDX, rIDX + length)\n",
    "\n",
    "               X = S_db[:, indexs]\n",
    "\n",
    "               NormX = NormalizeSpectrogram(X)\n",
    "               TrainDataList.append((NormX, genre))\n",
    "\n",
    "         #Validation data\n",
    "         elif trainSong > 80:\n",
    "            try:\n",
    "               y, sr = librosa.load(SongPath, sr=16000)\n",
    "               mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "               S_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "            except:\n",
    "               numErr += 1\n",
    "               continue \n",
    "\n",
    "            \n",
    "            #SeparateSong = np.shape(S_db)[1] // length\n",
    "            #for i in range(SeparateSong):\n",
    "               # indexs = np.arange(i * length, i * length + length)\n",
    "               # X = S_db[:, indexs]\n",
    "               # NormX = NormalizeSpectrogram(X)\n",
    "               # ValDataList.append((NormX, genre))\n",
    "            \n",
    "            for _ in range(8):\n",
    "\n",
    "               rIDX = np.random.randint(0, np.shape(S_db)[1] - length)\n",
    "               indexs = np.arange(rIDX, rIDX + length)\n",
    "\n",
    "               X = S_db[:, indexs]\n",
    "\n",
    "               NormX = NormalizeSpectrogram(X)\n",
    "               ValDataList.append((NormX, genre))\n",
    "\n",
    "\n",
    "   DataList.extend((TrainDataList, ValDataList))\n",
    "   return DataList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33aced47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataList = DataCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f3ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('YAMF/GenreCNN_Dataset1.pkl', 'wb') as f:\n",
    "#    pickle.dump(DataList, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0e1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreDataset(Dataset):\n",
    "    def __init__(self, path='YAMF/test.pkl', Train = True, transform=None):\n",
    "\n",
    "        with open(path, 'rb') as f:\n",
    "            TD = pickle.load(f)\n",
    "\n",
    "        if Train:\n",
    "            self.X = np.array([TD[0][i][0] for i in range(len(TD[0]))])\n",
    "            self.Y = np.array([TD[0][i][1] for i in range(len(TD[0]))])\n",
    "\n",
    "        else:\n",
    "            self.X = np.array([TD[1][i][0] for i in range(len(TD[1]))])\n",
    "            self.Y = np.array([TD[1][i][1] for i in range(len(TD[1]))])\n",
    "\n",
    "        del TD\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        xTensor = self.X[idx]\n",
    "        yTensor = self.Y[idx]\n",
    "\n",
    "        xTensor = torch.tensor(xTensor, dtype=torch.float32).unsqueeze(0)\n",
    "        return xTensor, torch.tensor(yTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d02b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData = GenreDataset(Train = True)\n",
    "# valData = GenreDataset(Train = False)\n",
    "# trainLoader = DataLoader(trainData, batch_size = 32, shuffle=True, num_workers=0)\n",
    "# valLoader = DataLoader(valData, batch_size = 32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90d3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreCNN(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),  # Force 4×4 output\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*4*64, 128),      # 4*4*128 = 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)  # AdaptiveAvgPool2d and Flatten are inside classifier\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5538df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "# from torch.optim import Adam\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = GenreCNN()\n",
    "# opt = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=3, factor=0.5)\n",
    "# loss_fn = CrossEntropyLoss()\n",
    "# model.to(device)\n",
    "\n",
    "# print(device)\n",
    "\n",
    "# epochs = 30\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "#     # Training phase\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "\n",
    "#     for batch_x, batch_y in tqdm(trainLoader):\n",
    "#         batch_x = batch_x.to(device)\n",
    "#         batch_y = batch_y.to(device)\n",
    "\n",
    "#         y_pred = model(batch_x)\n",
    "#         loss = loss_fn(y_pred, batch_y)\n",
    "\n",
    "#         opt.zero_grad()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(y_pred.data, 1)\n",
    "#         train_total += batch_y.size(0)\n",
    "#         train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "#     avg_train_loss = train_loss / len(trainLoader)\n",
    "#     train_acc = train_correct / train_total\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predictions = []\n",
    "#         true = []\n",
    "#         for batch_x, batch_y in tqdm(valLoader):\n",
    "#             batch_x = batch_x.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "#             y_pred = model(batch_x)\n",
    "#             predictions.append(y_pred)\n",
    "#             true.append(batch_y)\n",
    "\n",
    "#         predictions = torch.cat(predictions, axis=0)\n",
    "#         true = torch.cat(true, axis=0)\n",
    "#         val_loss = loss_fn(predictions, true)\n",
    "#         predicted_classes = torch.argmax(predictions, dim=1)\n",
    "#         val_acc = (predicted_classes == true).float().mean()\n",
    "\n",
    "#     # Store metrics\n",
    "#     train_losses.append(avg_train_loss)\n",
    "#     val_losses.append(val_loss.item())\n",
    "#     train_accuracies.append(train_acc)\n",
    "#     val_accuracies.append(val_acc.item())\n",
    "\n",
    "#     print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "#     print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b002d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf2 = 'FluidR3_GM/FluidR3_GM.sf2'\n",
    "pretty_midi.pretty_midi._SOUNDFONT = sf2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db77f4",
   "metadata": {},
   "source": [
    "### Another dataset with genre recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fddeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12c0d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IN PARALLEL\n",
    "\n",
    "ClassificationModel = GenreCNN(n_classes=10)  \n",
    "ClassificationModel.load_state_dict(torch.load('GenreCNN_Working.pth', map_location='cpu'))\n",
    "ClassificationModel.eval()  # Move outside for efficiency\n",
    "\n",
    "def Count(PredictedClass):\n",
    "    count = Counter(np.array(np.ravel(PredictedClass))).most_common(1)[0]\n",
    "    out = (count[0], np.round(count[1]/12, 2))\n",
    "    return out\n",
    "\n",
    "def LoadMidi(path):\n",
    "    try:\n",
    "        return pretty_midi.PrettyMIDI(path)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "import time\n",
    "def Classifier(InputPath):  # -1 = use all CPUs\n",
    "\n",
    "   sf2 = 'FluidR3_GM/FluidR3_GM.sf2'\n",
    "   pretty_midi.pretty_midi._SOUNDFONT = sf2 \n",
    "\n",
    "   GenreDict = {}\n",
    "\n",
    "   for dir in tqdm(os.listdir(InputPath)[0:50]):\n",
    "      DirPath = os.path.join(InputPath, dir)\n",
    "\n",
    "      if not os.path.isdir(DirPath):\n",
    "         continue\n",
    "\n",
    "      for file in os.listdir(DirPath):\n",
    "         if file.endswith('.mid'):\n",
    "\n",
    "            FilePath = os.path.join(DirPath, file)\n",
    "\n",
    "            midi_data = LoadMidi(FilePath)\n",
    "            if midi_data is None:\n",
    "               return (f'{dir}/{file}', None)\n",
    "\n",
    "            audio = midi_data.fluidsynth(fs= 22050)\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=22050, n_mels=128)\n",
    "            S_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "            PredictedClass = []\n",
    "            for _ in range(12):\n",
    "               rIDX = np.random.randint(0, S_db.shape[1] - 256)\n",
    "               X = S_db[:, rIDX:rIDX+256]\n",
    "\n",
    "               NormX = NormalizeSpectrogram(X)\n",
    "               xTensor = torch.tensor(NormX, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "               with torch.no_grad():\n",
    "                  y_pred = ClassificationModel(xTensor)\n",
    "                  predictions = torch.softmax(y_pred, dim=1)\n",
    "                  PredictedClass.append(torch.argmax(predictions, dim=1))\n",
    "            \n",
    "            \n",
    "            del S_db\n",
    "            gc.collect()\n",
    "            GenreDict[f'{dir}/{file[:-4]}'] =  Count(PredictedClass)\n",
    "\n",
    "   return GenreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94320f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:08<03:38,  4.54s/it]fluidsynth: error: There is no preset with bank number 128 and preset number 127 in SoundFont 1\n",
      " 24%|██▍       | 12/50 [02:18<09:07, 14.42s/it]fluidsynth: error: There is no preset with bank number 128 and preset number 1 in SoundFont 1\n",
      "fluidsynth: error: There is no preset with bank number 128 and preset number 1 in SoundFont 1\n",
      "fluidsynth: error: There is no preset with bank number 128 and preset number 1 in SoundFont 1\n",
      " 34%|███▍      | 17/50 [05:36<12:32, 22.80s/it]fluidsynth: error: There is no preset with bank number 128 and preset number 126 in SoundFont 1\n",
      "fluidsynth: error: There is no preset with bank number 128 and preset number 126 in SoundFont 1\n",
      "fluidsynth: error: There is no preset with bank number 128 and preset number 126 in SoundFont 1\n",
      " 34%|███▍      | 17/50 [07:46<15:05, 27.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m InputPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_midi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m test \u001b[38;5;241m=\u001b[39m Classifier(InputPath)\n",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m, in \u001b[0;36mClassifier\u001b[0;34m(InputPath)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m midi_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m audio \u001b[38;5;241m=\u001b[39m midi_data\u001b[38;5;241m.\u001b[39mfluidsynth(fs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m22050\u001b[39m)\n\u001b[1;32m     43\u001b[0m mel_spec \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m, n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     44\u001b[0m S_db \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mamplitude_to_db(mel_spec, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:974\u001b[0m, in \u001b[0;36mPrettyMIDI.fluidsynth\u001b[0;34m(self, fs, sf2_path)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# Get synthesized waveform for each instrument\u001b[39;00m\n\u001b[0;32m--> 974\u001b[0m waveforms \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mfluidsynth(fs\u001b[38;5;241m=\u001b[39mfs,\n\u001b[1;32m    975\u001b[0m                           sf2_path\u001b[38;5;241m=\u001b[39msf2_path) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruments]\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# Allocate output waveform, with #sample = max length of all waveforms\u001b[39;00m\n\u001b[1;32m    977\u001b[0m synthesized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(np\u001b[38;5;241m.\u001b[39mmax([w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m waveforms]))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/instrument.py:523\u001b[0m, in \u001b[0;36mInstrument.fluidsynth\u001b[0;34m(self, fs, sf2_path)\u001b[0m\n\u001b[1;32m    521\u001b[0m current_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fs\u001b[38;5;241m*\u001b[39mcurrent_time)\n\u001b[1;32m    522\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fs\u001b[38;5;241m*\u001b[39m(current_time \u001b[38;5;241m+\u001b[39m event[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m--> 523\u001b[0m samples \u001b[38;5;241m=\u001b[39m fl\u001b[38;5;241m.\u001b[39mget_samples(end \u001b[38;5;241m-\u001b[39m current_sample)[::\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    524\u001b[0m synthesized[current_sample:end] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m samples\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# Increment the current sample\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fluidsynth.py:1025\u001b[0m, in \u001b[0;36mSynth.get_samples\u001b[0;34m(self, len)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlen\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate audio samples\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03m    The return value will be a NumPy array containing the given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fluid_synth_write_s16_stereo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynth, \u001b[38;5;28mlen\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fluidsynth.py:668\u001b[0m, in \u001b[0;36mfluid_synth_write_s16_stereo\u001b[0;34m(synth, len)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m    667\u001b[0m buf \u001b[38;5;241m=\u001b[39m create_string_buffer(\u001b[38;5;28mlen\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 668\u001b[0m fluid_synth_write_s16(synth, \u001b[38;5;28mlen\u001b[39m, buf, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, buf, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(buf[:], dtype\u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mint16)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "InputPath = os.path.realpath('clean_midi')\n",
    "\n",
    "test = Classifier(InputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbdb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13817aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "import os, numpy as np\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "\n",
    "ClassificationModel = GenreCNN(n_classes=10)  \n",
    "ClassificationModel.load_state_dict(torch.load('GenreCNN_Working.pth', map_location='cpu'))\n",
    "ClassificationModel.eval()  # Move outside for efficiency\n",
    "\n",
    "def Count(PredictedClass):\n",
    "    count = Counter(np.array(np.ravel(PredictedClass))).most_common(1)[0]\n",
    "    out = (count[0], np.round(count[1]/12, 2))\n",
    "    return out\n",
    "\n",
    "def LoadMidi(path):\n",
    "    try:\n",
    "        return pretty_midi.PrettyMIDI(path)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def ProcessFile(dir, file, input_path):\n",
    "    FilePath = os.path.join(input_path, dir, file)\n",
    "\n",
    "    midi_data = LoadMidi(FilePath)\n",
    "    if midi_data is None:\n",
    "        return (f'{dir}/{file}', None)\n",
    "\n",
    "    audio = midi_data.fluidsynth(fs= 22050)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=22050, n_mels=128)\n",
    "    S_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    PredictedClass = []\n",
    "    for _ in range(12):\n",
    "        rIDX = np.random.randint(0, S_db.shape[1] - 256)\n",
    "        X = S_db[:, rIDX:rIDX+256]\n",
    "\n",
    "        NormX = NormalizeSpectrogram(X)\n",
    "        xTensor = torch.tensor(NormX, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = ClassificationModel(xTensor)\n",
    "            predictions = torch.softmax(y_pred, dim=1)\n",
    "            PredictedClass.append(torch.argmax(predictions, dim=1))\n",
    "\n",
    "    del S_db\n",
    "    gc.collect()\n",
    "\n",
    "    return (f'{dir}/{file[:-4]}', Count(PredictedClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def Classifier(InputPath, n_jobs=-1):  # -1 = use all CPUs\n",
    "\n",
    "    tasks = []\n",
    "    #50\n",
    "    for dir in os.listdir(InputPath)[0:50]:\n",
    "        DirPath = os.path.join(InputPath, dir)\n",
    "        if not os.path.isdir(DirPath):\n",
    "            continue\n",
    "        for file in os.listdir(DirPath):\n",
    "            if file.endswith('.mid'):\n",
    "                tasks.append((dir, file))\n",
    "\n",
    "    sf2 = 'FluidR3_GM/FluidR3_GM.sf2'\n",
    "    pretty_midi.pretty_midi._SOUNDFONT = sf2 \n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(ProcessFile)(dir, file, InputPath) for dir, file in tqdm(tasks)\n",
    "    )\n",
    "\n",
    "    GenreDict = {}\n",
    "    numErr = 0\n",
    "\n",
    "    for key, result in results:\n",
    "        if result is None:\n",
    "            numErr += 1\n",
    "        else:\n",
    "            GenreDict[key] = result\n",
    "\n",
    "    print(f\"Files with errors: {numErr}\")\n",
    "    return GenreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputPath = os.path.realpath('clean_midi')\n",
    "\n",
    "GenreDict = Classifier(InputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GenreDict9_1900-2077.npy', GenreDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "DatasetList = glob.glob('GenreDict*')\n",
    "GenreDataset = {}\n",
    "for file in DatasetList:\n",
    "   F = np.load(file, allow_pickle=True)\n",
    "   F = F.item()\n",
    "   GenreDataset.update(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e13ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GenreDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GenreDict.npy', GenreDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a431dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
