{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a872748",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import librosa.display\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "\n",
    "from Preprocessing import *\n",
    "from ExtractGenre import *\n",
    "\n",
    "import DatasetLoader as DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7f91075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleaningData(InputPath = os.path.realpath('clean_midi'), LogFolder = os.path.realpath('LogFolder'), FolderName = 'LogFolder'):\n",
    "\n",
    "   os.makedirs(FolderName, exist_ok=True)\n",
    "\n",
    "   for dirpath, dirnames, filenames in tqdm(os.walk(InputPath)):\n",
    "\n",
    "      midFile = [file for file in filenames if file.endswith('.mid')]\n",
    "      lenmidiFile = len(midFile)\n",
    "\n",
    "\n",
    "      if midFile:\n",
    "\n",
    "         for song in range(lenmidiFile):\n",
    "            songPath = os.path.join(dirpath, midFile[song])\n",
    "\n",
    "            mid = Func_CorruptedFile(songPath, songPath, midFile[song], LogFolder)\n",
    "\n",
    "            if mid is None:\n",
    "               continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46332it [1:11:42, 10.77it/s]\n"
     ]
    }
   ],
   "source": [
    "InputPath = os.path.realpath('CompleteMIDI')\n",
    "\n",
    "#CleaningData(InputPath, LogFolder='CompleteLogFolder', FolderName='CompleteLogFolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "237efb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SongID(InputPath):\n",
    "\n",
    "   #create a dictionary {SongID: Path_to_Song}\n",
    "   SongID_Dict = {}\n",
    "   for dirpath, dirnames, filenames in tqdm(os.walk(InputPath)):\n",
    "\n",
    "      midFile = [file for file in filenames if file.endswith('.mid')]\n",
    "      lenmidiFile = len(midFile)\n",
    "      \n",
    "      if midFile:\n",
    "         songID = os.path.basename(dirpath)\n",
    "         songPath = [os.path.join(dirpath, f) for f in midFile]\n",
    "\n",
    "         SongID_Dict[songID] = songPath[np.random.randint(0, lenmidiFile)]\n",
    "\n",
    "\n",
    "   #Transform the .cls file into a dictionary\n",
    "   clsPath = \"SongID-Genre.cls\"\n",
    "   genre_Dict = {}\n",
    "\n",
    "   with open(clsPath, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            song_id, genre = parts\n",
    "            genre_Dict[song_id] = genre\n",
    "\n",
    "   SongGenre_Dict = {}\n",
    "\n",
    "   for songID, path in SongID_Dict.items():\n",
    "      Genre = genre_Dict.get(songID)\n",
    "\n",
    "      if Genre:\n",
    "         SongGenre_Dict[(songID, Genre)] = path\n",
    "\n",
    "\n",
    "   return SongGenre_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9757099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46332it [00:03, 11821.37it/s]\n"
     ]
    }
   ],
   "source": [
    "InputPath = os.path.realpath('CompleteMIDI')\n",
    "\n",
    "SongGenre_Dict = SongID(InputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb232f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToFeedCNN(SongGenre_Dict, numSample = 1000):\n",
    "\n",
    "   it = 0\n",
    "   numErr = 0\n",
    "   Dataset = []\n",
    "   \n",
    "   keys = np.array(list(SongGenre_Dict))\n",
    "   rIDX = np.random.choice(np.arange(len(keys)), numSample)\n",
    "   for key in tqdm(keys[rIDX]):\n",
    "      \n",
    "      path = SongGenre_Dict[tuple(key)]\n",
    "\n",
    "      try:\n",
    "         with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "            midi = pretty_midi.PrettyMIDI(path)\n",
    "            audio = midi.fluidsynth(fs=16000, sf2_path=\"FluidR3_GM/FluidR3_GM.sf2\")\n",
    "\n",
    "            if audio is None or len(audio) == 0:\n",
    "               raise ValueError(\"Invalid audio synthesized from MIDI.\")\n",
    "\n",
    "            S = librosa.feature.melspectrogram(y=audio, sr=16000, n_mels=128)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "      except RuntimeWarning as w:\n",
    "         numErr += 1\n",
    "         continue\n",
    "\n",
    "      except Exception as e:\n",
    "         numErr += 1\n",
    "         continue  \n",
    "\n",
    "\n",
    "      #Data augmenting\n",
    "      if key[1] not in ['Electronic', 'Pop_Rock']:\n",
    "         for _ in range(15):\n",
    "            #Choose a random 256 array in the S_dB (not considered the whole song, only a fraction)\n",
    "            rNumber = np.random.randint(0, S_dB.shape[1] - 256)\n",
    "            idx = np.arange(rNumber, rNumber + 256)\n",
    "            X = S_dB[:, idx]\n",
    "            Y = key[1]\n",
    "            Dataset.append((X, Y))\n",
    "\n",
    "      else:\n",
    "         #Choose a random 256 array in the S_dB (not considered the whole song, only a fraction)\n",
    "         rNumber = np.random.randint(0, S_dB.shape[1] - 256)\n",
    "         idx = np.arange(rNumber, rNumber + 256)\n",
    "         X = S_dB[:, idx]\n",
    "         Y = key[1]\n",
    "         Dataset.append((X, Y))\n",
    "\n",
    "      it += 1\n",
    "\n",
    "      if it > numSample:\n",
    "         break\n",
    "\n",
    "   print(numErr)\n",
    "\n",
    "   return Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datas = ToFeedCNN(SongGenre_Dict, numSample = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c927841",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GenreTrainDataset/test.pkl', 'wb') as f:\n",
    "   pickle.dump(Datas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea0d0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNPreprocessing(GenreTrainingPath):\n",
    "\n",
    "    # Load the file in the folder\n",
    "    GenreTrainingDS = []\n",
    "    for file in os.listdir(GenreTrainingPath):\n",
    "        filePath = os.path.join(GenreTrainingPath, file)\n",
    "        if file[:4] == 'test':\n",
    "            with open(filePath, 'rb') as f:\n",
    "                TD = pickle.load(f)\n",
    "            GenreTrainingDS.extend(TD)\n",
    "\n",
    "    # Maps each genre into a number\n",
    "    GenreMapping = {\n",
    "        'Vocal': 0, 'Pop_Rock': 1, 'Latin': 2, 'Electronic': 3, 'Country': 4, 'Reggae': 5, 'Rap': 6,\n",
    "        'RnB': 7, 'Jazz': 8, 'Folk': 9, 'Religious': 10, 'Classical': 11, 'Easy_Listening': 12,\n",
    "        'International': 13,\n",
    "    }\n",
    "\n",
    "    # First pass: collect all features for fitting scaler\n",
    "    all_features = []\n",
    "    valid_songs = []  # store songs that don't raise errors\n",
    "    for song in GenreTrainingDS:\n",
    "        try:\n",
    "            X = song[0]  # shape (128, 256)\n",
    "            if X.shape[1] != 256:  # skip corrupt entries\n",
    "                continue\n",
    "            all_features.append(X.reshape(-1, X.shape[1]))  # shape (128, 256)\n",
    "            valid_songs.append(song)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    all_features = np.vstack(all_features)  # shape (N * 128, 256)\n",
    "    scaler = StandardScaler().fit(all_features)\n",
    "\n",
    "    # Second pass: scale using the fitted scaler\n",
    "    TrainDataset = []\n",
    "    for song in valid_songs:\n",
    "        try:\n",
    "            X = scaler.transform(song[0])  # still shape (128, 256)\n",
    "            Y = GenreMapping[song[1]]\n",
    "            TrainDataset.append((X, Y))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    del GenreTrainingDS\n",
    "    gc.collect()\n",
    "\n",
    "    return TrainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c7e4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenreTrainingPath = os.path.realpath('GenreTrainDataset')\n",
    "TrainDataset = CNNPreprocessing(GenreTrainingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991718b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('GenreTrainDataset/CNNDataset.pkl', 'wb') as f:\n",
    "#    pickle.dump(TrainDataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36b4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class GenreDataset(Dataset):\n",
    "    def __init__(self, path='GenreTrainDataset/CNNDataset.pkl', Train = True, transform=None):\n",
    "        # self.file = h5py.File(path, 'r')\n",
    "        # self.X = self.file['x']\n",
    "        # self.Y = self.file['y']\n",
    "\n",
    "        with open(path, 'rb') as f:\n",
    "            TD = pickle.load(f)\n",
    "\n",
    "\n",
    "        TrainIDX = np.arange(10_000)\n",
    "        ValIDX = np.arange(len(TrainIDX), len(TrainIDX) + 3_000)\n",
    "\n",
    "        if Train:\n",
    "            self.X = np.array([TD[i][0] for i in TrainIDX])\n",
    "            self.Y = np.array([TD[i][1] for i in TrainIDX])\n",
    "\n",
    "        else:\n",
    "            self.X = np.array([TD[i][0] for i in ValIDX])\n",
    "            self.Y = np.array([TD[i][1] for i in ValIDX])\n",
    "\n",
    "        del TD\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        xTensor = self.X[idx]\n",
    "        yTensor = self.Y[idx]\n",
    "\n",
    "        xTensor = torch.tensor(xTensor, dtype=torch.float32).unsqueeze(0)\n",
    "        return xTensor, torch.tensor(yTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833c8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = GenreDataset(path='GenreTrainDataset/CNNDataset.pkl', Train = True)\n",
    "valData = GenreDataset(path='GenreTrainDataset/CNNDataset.pkl', Train = False)\n",
    "trainLoader = DataLoader(trainData, batch_size = 32, shuffle=True, num_workers=0)\n",
    "valLoader = DataLoader(valData, batch_size = 32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d280ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GenreCNN(nn.Module):\n",
    "    def __init__(self, n_classes=12, input_shape=(1, 128, 256)):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Infer the flattened size after convolutions\n",
    "        self._to_linear = self._get_conv_output_size(input_shape)\n",
    "        self.fc = nn.Linear(self._to_linear, n_classes)\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            x = self.pool(F.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            x = self.pool(F.relu(self.conv4(x)))\n",
    "            return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc40433",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGenreCNNSimple\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class GenreCNNSimple(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),  # Force 4Ã—4 output\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*4*128, 256),      # 4*4*128 = 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)  # AdaptiveAvgPool2d and Flatten are inside classifier\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GenreCNNSimple()\n",
    "opt = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=3, factor=0.5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "print(device)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch_x, batch_y in tqdm(trainLoader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        y_pred = model(batch_x)\n",
    "        loss = loss_fn(y_pred, batch_y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    # Calculate epoch averages\n",
    "    avg_train_loss = train_loss / len(trainLoader)\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Validation phase (your existing code)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in tqdm(valLoader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            y_pred = model(batch_x)\n",
    "            predictions.append(y_pred)\n",
    "            true.append(batch_y)\n",
    "\n",
    "        predictions = torch.cat(predictions, axis=0)\n",
    "        true = torch.cat(true, axis=0)\n",
    "        val_loss = loss_fn(predictions, true)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "        val_acc = (predicted_classes == true).float().mean()\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss.item())\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc.item())\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c51021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
