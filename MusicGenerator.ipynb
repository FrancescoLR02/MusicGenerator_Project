{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from Preprocessing import *\n",
    "from CNN_ExtractGenre import *\n",
    "from PolyphonicPreprocessing import *\n",
    "\n",
    "import DatasetLoader as DL\n",
    "import Model as M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daceff",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Apply the functions in Preprocessing.py to clean the midi dataset. There are multiple files that are currupted or duplicated. \n",
    "For this analysis we are also going to use only Midi file with a timestamp of 4/4, like in the reference paper. This filtering is done in CleaningData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3973b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting Duplicates:: 100%|██████████| 2201/2201 [00:00<00:00, 2227.41it/s]\n",
      "100%|██████████| 2201/2201 [06:27<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "DeleteDuplicates()\n",
    "CleaningData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82aec08",
   "metadata": {},
   "source": [
    "# Preprocessing data:\n",
    "\n",
    "Firstly we reconstruct the database, transforming all the polyphonic audios into monophonic, keeping the information about the tracks (instruments) in each of the midi file. It is done by keeping only the highest pitch from each polyphonic note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad18c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recreating Database: 100%|██████████| 2059/2059 [08:41<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "RecreateDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33b80b",
   "metadata": {},
   "source": [
    "The input of the model has to be a 128x16 matrix as in the paper. The following function clasify the midi tracks into instrumental classes:\n",
    "- Guitar  \n",
    "- Percussion\n",
    "- Organ  \n",
    "- Sound Effects \n",
    "- Bass  \n",
    "- Piano \n",
    "- Synth Lead \n",
    "- Chromatic Percussion \n",
    "- Synth Pad  \n",
    "- Percussive \n",
    "- Synth Effects\n",
    "- Ethnic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 1000/1000 [01:14<00:00, 13.39it/s]\n"
     ]
    }
   ],
   "source": [
    "Dataset = PreProcessing(nDir = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bass  43862\n",
      "Guitar  74291\n",
      "Chromatic Percussion  10003\n",
      "Organ  15504\n",
      "Reed  12604\n",
      "Strings  10644\n",
      "Pipe  7794\n",
      "Sound Effects  2797\n",
      "Ensemble  19488\n",
      "Synth Lead  8047\n",
      "Piano  23527\n",
      "Brass  13247\n",
      "Ethnic  1539\n",
      "Synth Pad  4020\n",
      "Percussive  2806\n",
      "Synth Effects  1984\n"
     ]
    }
   ],
   "source": [
    "#torch.save(Dataset, 'Dataset.pt')\n",
    "\n",
    "Dataset = torch.load('Dataset.pt', map_location='cpu')\n",
    "\n",
    "for key, value in Dataset.items():\n",
    "   print(key, '', len(value['Tempo']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb5535",
   "metadata": {},
   "source": [
    "# Monophonic Model and Architecture\n",
    "\n",
    "The class DatasetTransorm allow us to choose which intrument's bars to load. We can load from the simple dataset (1) or from (2) or (3), specifying the genre and the instrument's bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09475244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are selecting the data from the (3) dataset, rocks song played with string instruments\n",
    "Data = DL.DatasetTransform(Genre = False, GenreID = None, Instrument='Guitar')\n",
    "trainData = DataLoader(Data, batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31f816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65bb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d495b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56575f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4e93f5",
   "metadata": {},
   "source": [
    "# Genre detection using CNN\n",
    "\n",
    "The idea is to train a Convolutional Neural Network (CNN) to understand the structure of the songs and to implement a classifier capable of identifying the genre of each song in our MIDI dataset.\n",
    "\n",
    "However, we cannot train the CNN directly on our MIDI dataset, since this would compromise both learning and classification. Moreover, CNNs are supervised learning models, and our dataset does not include genre labels. For this reason, we found another dataset containing 100 songs in .wav format for each of the following musical genres:\n",
    "\n",
    "- metal\n",
    "- disco\n",
    "- classical\n",
    "- hiphop\n",
    "- jazz\n",
    "- country\n",
    "- pop\n",
    "- blues \n",
    "- raggae \n",
    "- rock\n",
    "\n",
    "The idea is to train the CNN using this labeled dataset. Before doing that, we need to perform some preprocessing, since some songs in the dataset are corrupted. Additionally, the audio clips are only a few seconds long, so we preprocess each song to have a fixed length and a consistent format. The preprocessing functions are implemented in the file **CNN_ExtractGenre.py**.\n",
    "\n",
    "After preprocessing, we define the CNN model and the data loader in **Model.py** and **DataLoader.py**, respectively. The model is trained on Google Colab (not on the local machine), and we later load the trained model using its state_dict.\n",
    "\n",
    "The CNN achieves a strong validation accuracy of 84%, as shown in the accompanying paper.\n",
    "\n",
    "Once the model is trained on the labeled dataset, we use it to classify our own songs. This is a complex process because our songs are in .mid format, while the model expects .wav spectrograms as input. Therefore, each MIDI file must be converted into audio, transformed into a spectrogram, and then classified by the CNN.\n",
    "\n",
    "After classifying each song, we save a file containing the song’s name and its predicted genre. From there, we proceed as before: we separate our dataset by genre, and within each genre, we further separate the songs by instrument.\n",
    "\n",
    "\n",
    "N.B. all the function in the file **CNN_ExtractGenre** has already been runned since the computation is quite long. In the following cell we are showing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30a7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gordon Lightfoot/Sundown (9, 0.5)\n",
      "Gordon Lightfoot/Sundown.1 (9, 1.0)\n",
      "Gordon Lightfoot/Rainy Day People (9, 0.5)\n",
      "Gordon Lightfoot/Carefree Highway (9, 0.92)\n",
      "Gordon Lightfoot/Beautiful (4, 0.67)\n",
      "Gounod Charles/Ave Maria.1 (2, 0.92)\n",
      "Gounod Charles/Marche funebre d'une marionnette (2, 0.42)\n",
      "Gounod Charles/Waltz from Faust (2, 1.0)\n",
      "Grace Jones/Slave to the Rhythm (6, 0.58)\n",
      "Grand Funk Railroad/Some Kind of Wonderful (1, 1.0)\n",
      "Grand Funk Railroad/I'm Your Captain (Closer to Home) (9, 0.67)\n"
     ]
    }
   ],
   "source": [
    "#Load the preprocessed and classified dataset:\n",
    "with open('GenreDataset.pkl', 'rb') as f:\n",
    "   GenreDataset = pickle.load(f)\n",
    "\n",
    "#Mapping each genre into a number for classification\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "i = 0\n",
    "for key in GenreDataset.keys():\n",
    "   i += 1\n",
    "   print(key, GenreDataset[key])\n",
    "\n",
    "   if i > 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e127",
   "metadata": {},
   "source": [
    "# Polyphonic Music Generator\n",
    "\n",
    "as now we are only considering monophonic tracks, we are thus losing all the information between notes of the same instrument and the correlation between instruments! Therefore we would like to try to implement a polyphonic music generator. \n",
    "\n",
    "The strategy is the same as before. Instead of having (128x16) matrix we have (4x128x16) where 4 is the maximum number of instruments that can be played at the same time. Now each matrix 128x16 does not encode for a single note as before, but it allows for multiple note of the same instrument.\n",
    "\n",
    "In the file **PolyphonicPreprocessing.py** there are all the function used to preprocess the clean_midi dataset and build the dataset of mapped songs, cathegorized by genre (using the genre recognition dataset built before). Here we are processing and storing the Polyphonic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ecb3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [08:10<00:00,  4.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "PolyphonicDataset = PolyphonicPreProcessing(nDir = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(PolyphonicDataset, 'PolyphonicDataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddd389",
   "metadata": {},
   "source": [
    "And these are all the genre and the number of bars for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b133b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disco  12814\n",
      "rock  60419\n",
      "country  10710\n",
      "pop  8103\n",
      "classical  9710\n",
      "blues  2088\n",
      "jazz  8672\n",
      "raggae  3019\n",
      "hiphop  255\n",
      "metal  141\n"
     ]
    }
   ],
   "source": [
    "#PolyphonicDataset = torch.load('PolyphonicDataset.pt')\n",
    "\n",
    "for key, value in PolyphonicDataset.items():\n",
    "   print(key, '', len(value['Tempo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca5212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
