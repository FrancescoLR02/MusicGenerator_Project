{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d021b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from Preprocessing import *\n",
    "from ExtractGenre import *\n",
    "from CNN_ExtractGenre import *\n",
    "\n",
    "import DatasetLoader as DL\n",
    "import Model as M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daceff",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Apply the functions in Preprocessing.py to clean the midi dataset. There are multiple files that are currupted or duplicated. \n",
    "For this analysis we are also going to use only Midi file with a timestamp of 4/4, like in the reference paper. This filtering is done in CleaningData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3973b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting Duplicates:: 100%|██████████| 2201/2201 [00:00<00:00, 2227.41it/s]\n",
      "100%|██████████| 2201/2201 [06:27<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "DeleteDuplicates()\n",
    "CleaningData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82aec08",
   "metadata": {},
   "source": [
    "# Preprocessing data:\n",
    "\n",
    "Firstly we reconstruct the database, transforming all the polyphonic audios into monophonic, keeping the information about the tracks (instruments) in each of the midi file. It is done by keeping only the highest pitch from each polyphonic note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad18c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recreating Database: 100%|██████████| 2059/2059 [08:41<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "RecreateDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33b80b",
   "metadata": {},
   "source": [
    "The input of the model has to be a 128x16 matrix as in the paper. The following function clasify the midi tracks into instrumental classes:\n",
    "- Guitar  \n",
    "- Percussion\n",
    "- Organ  \n",
    "- Sound Effects \n",
    "- Bass  \n",
    "- Piano \n",
    "- Synth Lead \n",
    "- Chromatic Percussion \n",
    "- Synth Pad  \n",
    "- Percussive \n",
    "- Synth Effects\n",
    "- Ethnic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2059/2059 [03:21<00:00, 10.23it/s]\n",
      "Remapping: 100%|██████████| 2907/2907 [00:00<00:00, 15627.60it/s]\n"
     ]
    }
   ],
   "source": [
    "Dataset = PreProcessing(nDir = 2059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2403bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guitar  334698\n",
      "Percussion  252661\n",
      "Organ  59214\n",
      "Sound Effects  8534\n",
      "Bass  103949\n",
      "Piano  82839\n",
      "Synth Lead  18431\n",
      "Chromatic Percussion  46655\n",
      "Synth Pad  7256\n",
      "Percussive  9855\n",
      "Synth Effects  5615\n",
      "Ethnic  7377\n"
     ]
    }
   ],
   "source": [
    "#torch.save(Dataset, 'Dataset.pt')\n",
    "\n",
    "Dataset = torch.load('Dataset.pt', map_location='cpu')\n",
    "\n",
    "for key, value in Dataset.items():\n",
    "   print(key, '', len(value['Tempo']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e93f5",
   "metadata": {},
   "source": [
    "# Genre detection using CNN\n",
    "\n",
    "The idea is to train a Convolutional Neural Network (CNN) to understand the structure of the songs and to implement a classifier capable of identifying the genre of each song in our MIDI dataset.\n",
    "\n",
    "However, we cannot train the CNN directly on our MIDI dataset, since this would compromise both learning and classification. Moreover, CNNs are supervised learning models, and our dataset does not include genre labels. For this reason, we found another dataset containing 100 songs in .wav format for each of the following musical genres:\n",
    "\n",
    "- metal\n",
    "- disco\n",
    "- classical\n",
    "- hiphop\n",
    "- jazz\n",
    "- country\n",
    "- pop\n",
    "- blues \n",
    "- raggae \n",
    "- rock\n",
    "\n",
    "The idea is to train the CNN using this labeled dataset. Before doing that, we need to perform some preprocessing, since some songs in the dataset are corrupted. Additionally, the audio clips are only a few seconds long, so we preprocess each song to have a fixed length and a consistent format. The preprocessing functions are implemented in the file **CNN_ExtractGenre.py**.\n",
    "\n",
    "After preprocessing, we define the CNN model and the data loader in **Model.py** and **DataLoader.py**, respectively. The model is trained on Google Colab (not on the local machine), and we later load the trained model using its state_dict.\n",
    "\n",
    "The CNN achieves a strong validation accuracy of 84%, as shown in the accompanying paper.\n",
    "\n",
    "Once the model is trained on the labeled dataset, we use it to classify our own songs. This is a complex process because our songs are in .mid format, while the model expects .wav spectrograms as input. Therefore, each MIDI file must be converted into audio, transformed into a spectrogram, and then classified by the CNN.\n",
    "\n",
    "After classifying each song, we save a file containing the song’s name and its predicted genre. From there, we proceed as before: we separate our dataset by genre, and within each genre, we further separate the songs by instrument.\n",
    "\n",
    "\n",
    "N.B. all the function in the file **CNN_ExtractGenre** has already been runned since the computation is quite long. In the following cell we are showing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a7966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6495, 128, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the preprocessed and classified dataset:\n",
    "with open('CNN_GenreDataset.pkl', 'rb') as f:\n",
    "   CNN_GenreDataset = pickle.load(f)\n",
    "\n",
    "#Mapping each genre into a number for classification\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "\n",
    "#Visualizing the shape of the dataset bars for rock songs, with string instrument \n",
    "np.shape(CNN_GenreDataset[9]['String']['Bars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e127",
   "metadata": {},
   "source": [
    "We now have three datasets available to train the MidiNet model:\n",
    "1. Instrument-Only Dataset\n",
    "This is the simplest version, where each song is categorized solely based on the instrument(s) played.\n",
    "2.\tClustering-Based Genre Dataset\n",
    "In this version, songs are first grouped by genre using clustering techniques, based on simple extracted features. Within each genre cluster, the songs are further categorized by instrument.\n",
    "3. CNN-Based Genre Dataset\n",
    "This version uses a Convolutional Neural Network (CNN) to classify each original song in the dataset by genre. Once a genre label is assigned, each song is then placed into its corresponding genre category and further divided by instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb3493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afd910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b133b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca5212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2563528",
   "metadata": {},
   "source": [
    "# Model and Architecture\n",
    "\n",
    "The class DatasetTransorm allow us to choose which intrument's bars to load. We can load from the simple dataset (1) or from (2) or (3), specifying the genre and the instrument's bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1cce35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are selecting the data from the (3) dataset, rocks song played with string instruments\n",
    "Data = DL.DatasetTransform(Genre = False, GenreID = None, Instrument='Guitar')\n",
    "trainData = DataLoader(Data, batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956bfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
