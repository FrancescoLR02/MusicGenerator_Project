{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d021b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from Preprocessing import *\n",
    "from CNN_ExtractGenre import *\n",
    "from PolyphonicPreprocessing import *\n",
    "\n",
    "import DatasetLoader as DL\n",
    "import Model as M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daceff",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Apply the functions in Preprocessing.py to clean the midi dataset. There are multiple files that are currupted or duplicated. \n",
    "For this analysis we are also going to use only Midi file with a timestamp of 4/4, like in the reference paper. This filtering is done in CleaningData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3973b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting Duplicates:: 100%|██████████| 2201/2201 [00:00<00:00, 2227.41it/s]\n",
      "100%|██████████| 2201/2201 [06:27<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "DeleteDuplicates()\n",
    "CleaningData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33b80b",
   "metadata": {},
   "source": [
    "The input of the model has to be a 128x16 matrix as in the paper. The following function clasify the midi tracks into instrumental classes:\n",
    "- Guitar  \n",
    "- Percussion\n",
    "- Organ  \n",
    "- Sound Effects \n",
    "- Bass  \n",
    "- Piano \n",
    "- Synth Lead \n",
    "- Chromatic Percussion \n",
    "- Synth Pad  \n",
    "- Percussive \n",
    "- Synth Effects\n",
    "- Ethnic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [07:57<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "MonophonicDataset = PreProcessing(nDir = 2000)\n",
    "torch.save(MonophonicDataset, 'MonophonicDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a64488",
   "metadata": {},
   "outputs": [],
   "source": [
    "MonophonicDataset = torch.load('MonophonicDataset.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2403bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bass                      number of Bars: 106314\n",
      "Ensemble                  number of Bars: 53067\n",
      "Organ                     number of Bars: 37496\n",
      "Guitar                    number of Bars: 176320\n",
      "Piano                     number of Bars: 48277\n",
      "Synth Lead                number of Bars: 18727\n",
      "Percussive                number of Bars: 4689\n",
      "Synth Effects             number of Bars: 6506\n",
      "Reed                      number of Bars: 28849\n",
      "Brass                     number of Bars: 31106\n",
      "Synth Pad                 number of Bars: 13475\n",
      "Pipe                      number of Bars: 16226\n",
      "Strings                   number of Bars: 27713\n",
      "Sound Effects             number of Bars: 7644\n",
      "Chromatic Percussion      number of Bars: 25030\n",
      "Percussion                number of Bars: 3890\n",
      "Ethnic                    number of Bars: 2970\n"
     ]
    }
   ],
   "source": [
    "for key in MonophonicDataset.keys():\n",
    "   print(f\"{key.ljust(25)} number of Bars: {len(MonophonicDataset[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a74096a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SongName': (\"It's a Real Good Feeling\", \"It's a Real Good Feeling\"),\n",
       " 'Bars': (tensor(indices=tensor([[29, 29, 29, 29, 29, 29, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "                          38, 38, 38, 38],\n",
       "                         [ 0,  1,  2,  3,  4,  5,  5,  6,  7, 11, 12, 13, 14, 15,\n",
       "                           8,  9, 10, 11]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "         size=(128, 16), nnz=18, dtype=torch.int32, layout=torch.sparse_coo),\n",
       "  tensor(indices=tensor([[34, 34, 34, 34, 34, 34, 41, 41, 41, 41, 41, 41, 41, 43,\n",
       "                          43, 43, 43],\n",
       "                         [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 12, 13, 14, 15,  8,\n",
       "                           9, 10, 11]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "         size=(128, 16), nnz=17, dtype=torch.int32, layout=torch.sparse_coo)),\n",
       " 'Program': 35,\n",
       " 'Channel': (1, 1),\n",
       " 'numBar': (2, 3),\n",
       " 'Tempo': (123, 123)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First element of the Bass instruments. There are all the possible information needed\n",
    "MonophonicDataset['Bass'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125542de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we load it after, we free some space\n",
    "del MonophonicDataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb5535",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09475244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d495b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56575f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4e93f5",
   "metadata": {},
   "source": [
    "# Genre detection using CNN\n",
    "\n",
    "The idea is to train a Convolutional Neural Network (CNN) to understand the structure of the songs and to implement a classifier capable of identifying the genre of each song in our MIDI dataset.\n",
    "\n",
    "However, we cannot train the CNN directly on our MIDI dataset, since this would compromise both learning and classification. Moreover, CNNs are supervised learning models, and our dataset does not include genre labels. For this reason, we found another dataset containing 100 songs in .wav format for each of the following musical genres:\n",
    "\n",
    "- metal\n",
    "- disco\n",
    "- classical\n",
    "- hiphop\n",
    "- jazz\n",
    "- country\n",
    "- pop\n",
    "- blues \n",
    "- raggae \n",
    "- rock\n",
    "\n",
    "The idea is to train the CNN using this labeled dataset. Before doing that, we need to perform some preprocessing, since some songs in the dataset are corrupted. Additionally, the audio clips are only a few seconds long, so we preprocess each song to have a fixed length and a consistent format. The preprocessing functions are implemented in the file **CNN_ExtractGenre.py**.\n",
    "\n",
    "After preprocessing, we define the CNN model and the data loader in **Model.py** and **DataLoader.py**, respectively. The model is trained on Google Colab (not on the local machine), and we later load the trained model using its state_dict.\n",
    "\n",
    "The CNN achieves a strong validation accuracy of 84%, as shown in the accompanying paper.\n",
    "\n",
    "Once the model is trained on the labeled dataset, we use it to classify our own songs. This is a complex process because our songs are in .mid format, while the model expects .wav spectrograms as input. Therefore, each MIDI file must be converted into audio, transformed into a spectrogram, and then classified by the CNN.\n",
    "\n",
    "After classifying each song, we save a file containing the song’s name and its predicted genre. From there, we proceed as before: we separate our dataset by genre, and within each genre, we further separate the songs by instrument.\n",
    "\n",
    "\n",
    "N.B. all the function in the file **CNN_ExtractGenre** has already been runned since the computation is quite long. In the following cell we are showing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c30a7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song name: Gordon Lightfoot/Sundown                                     Genre and probability:    (9, 0.50)\n",
      "Song name: Gordon Lightfoot/Sundown.1                                   Genre and probability:    (9, 1.00)\n",
      "Song name: Gordon Lightfoot/Rainy Day People                            Genre and probability:    (9, 0.50)\n",
      "Song name: Gordon Lightfoot/Carefree Highway                            Genre and probability:    (9, 0.92)\n",
      "Song name: Gordon Lightfoot/Beautiful                                   Genre and probability:    (4, 0.67)\n",
      "Song name: Gounod Charles/Ave Maria.1                                   Genre and probability:    (2, 0.92)\n",
      "Song name: Gounod Charles/Marche funebre d'une marionnette              Genre and probability:    (2, 0.42)\n",
      "Song name: Gounod Charles/Waltz from Faust                              Genre and probability:    (2, 1.00)\n",
      "Song name: Grace Jones/Slave to the Rhythm                              Genre and probability:    (6, 0.58)\n",
      "Song name: Grand Funk Railroad/Some Kind of Wonderful                   Genre and probability:    (1, 1.00)\n",
      "Song name: Grand Funk Railroad/I'm Your Captain (Closer to Home)        Genre and probability:    (9, 0.67)\n"
     ]
    }
   ],
   "source": [
    "#Load the preprocessed and classified dataset:\n",
    "with open('GenreDataset.pkl', 'rb') as f:\n",
    "   GenreDataset = pickle.load(f)\n",
    "\n",
    "#Mapping each genre into a number for classification\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "i = 0\n",
    "for key in GenreDataset.keys():\n",
    "   i += 1\n",
    "   genre, prob = GenreDataset[key]\n",
    "   print(f\"{'Song name:':<10} {key.ljust(60)} {'Genre and probability:':<25} ({genre}, {prob:.2f})\")\n",
    "\n",
    "   if i > 10:\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e127",
   "metadata": {},
   "source": [
    "# Polyphonic Music Generator\n",
    "\n",
    "as now we are only considering monophonic tracks, we are thus losing all the information between notes of the same instrument and the correlation between instruments! Therefore we would like to try to implement a polyphonic music generator. \n",
    "\n",
    "The strategy is the same as before. Instead of having (128x16) matrix we have (4x128x16) where 4 is the maximum number of instruments that can be played at the same time. Now each matrix 128x16 does not encode for a single note as before, but it allows for multiple note of the same instrument.\n",
    "\n",
    "In the file **PolyphonicPreprocessing.py** there are all the function used to preprocess the clean_midi dataset and build the dataset of mapped songs, cathegorized by genre (using the genre recognition dataset built before). Here we are processing and storing the Polyphonic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ecb3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [13:25<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900\n"
     ]
    }
   ],
   "source": [
    "PolyphonicDataset = PolyphonicPreProcessing(nDir = 2000)\n",
    "torch.save(PolyphonicDataset, 'PolyphonicDataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddd389",
   "metadata": {},
   "source": [
    "And these are all the genre and the number of bars for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "294b81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolyphonicDataset = torch.load('PolyphonicDataset.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9dffc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SongName': ('The Righteous Brothers/Unchained Melody.3',\n",
       "  'The Righteous Brothers/Unchained Melody.3'),\n",
       " 'Bars': (tensor(indices=tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "                           0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1],\n",
       "                         [31, 33, 33, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "                          36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 37, 37,\n",
       "                          42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "                          42, 42],\n",
       "                         [ 6,  6,  7,  0,  1,  2,  3,  4,  5,  8,  9, 10, 11, 12,\n",
       "                          13, 14,  0,  1,  6,  7,  8,  9, 14, 15,  4,  5, 12, 13,\n",
       "                           0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                          14, 15]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1]),\n",
       "         size=(4, 128, 16), nnz=44, layout=torch.sparse_coo),\n",
       "  tensor(indices=tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "                           0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1],\n",
       "                         [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "                          33, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 37, 37, 42,\n",
       "                          42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "                          42, 46],\n",
       "                         [ 0,  1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14,\n",
       "                          15,  0,  1,  6,  7,  8,  9, 14, 15,  4,  5, 12, 13,  0,\n",
       "                           1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "                          15, 15]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1]),\n",
       "         size=(4, 128, 16), nnz=44, layout=torch.sparse_coo)),\n",
       " 'Program': [32, 150, 48, 48],\n",
       " 'ActiveProgram': (array([1, 1, 0, 0]), array([1, 1, 0, 0])),\n",
       " 'numBar': (2, 3),\n",
       " 'Tempo': 80,\n",
       " 'Genre': 9}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolyphonicDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e19551",
   "metadata": {},
   "outputs": [],
   "source": [
    "del PolyphonicDataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d793b2",
   "metadata": {},
   "source": [
    "# Monophonic Model and Architecture\n",
    "\n",
    "The class DatasetTransorm allow us to choose which intrument's bars to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are selecting the data from the dataset with the guitar instrument\n",
    "Data = DL.MonophonicDataset(Instrument='Guitar')\n",
    "Bars, PreviousBars, Cond1D = DataLoader(Data, batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452518b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolData = DL.PolyphonicDataset(Genre = 'jazz')\n",
    "PolTrainData = DataLoader(PolData, batch_size=30, shuffle=True, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
