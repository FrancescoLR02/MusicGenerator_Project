{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d021b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from Preprocessing import *\n",
    "from CNN_ExtractGenre import *\n",
    "from PolyphonicPreprocessing import *\n",
    "\n",
    "from DatasetLoader import collate_fn as cf\n",
    "import DatasetLoader as DL\n",
    "import Model as M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daceff",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Apply the functions in Preprocessing.py to clean the midi dataset. There are multiple files that are currupted or duplicated. \n",
    "For this analysis we are also going to use only Midi file with a timestamp of 4/4, like in the reference paper. This filtering is done in CleaningData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3973b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting Duplicates:: 100%|██████████| 2201/2201 [00:00<00:00, 2227.41it/s]\n",
      "100%|██████████| 2201/2201 [06:27<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "DeleteDuplicates()\n",
    "CleaningData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82aec08",
   "metadata": {},
   "source": [
    "# Preprocessing data:\n",
    "\n",
    "Firstly we reconstruct the database, transforming all the polyphonic audios into monophonic, keeping the information about the tracks (instruments) in each of the midi file. It is done by keeping only the highest pitch from each polyphonic note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recreating Database: 100%|██████████| 2059/2059 [08:41<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#RecreateDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33b80b",
   "metadata": {},
   "source": [
    "The input of the model has to be a 128x16 matrix as in the paper. The following function clasify the midi tracks into instrumental classes:\n",
    "- Guitar  \n",
    "- Percussion\n",
    "- Organ  \n",
    "- Sound Effects \n",
    "- Bass  \n",
    "- Piano \n",
    "- Synth Lead \n",
    "- Chromatic Percussion \n",
    "- Synth Pad  \n",
    "- Percussive \n",
    "- Synth Effects\n",
    "- Ethnic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  60%|█████▉    | 1192/2000 [04:44<03:12,  4.20it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "MThd not found. Probably not a MIDI file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Dataset \u001b[38;5;241m=\u001b[39m PreProcessing(nDir \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(Dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset_Percussion1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Università/PhysicsOfData/Neural Network and Deep Learning/MusicGenerator_Project/Preprocessing.py:440\u001b[0m, in \u001b[0;36mPreProcessing\u001b[0;34m(nDir, Velocity)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFolder/Debug.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    438\u001b[0m    f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 440\u001b[0m mid \u001b[38;5;241m=\u001b[39m mido\u001b[38;5;241m.\u001b[39mMidiFile(FilePath)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:320\u001b[0m, in \u001b[0;36mMidiFile.__init__\u001b[0;34m(self, filename, file, type, ticks_per_beat, charset, debug, clip, tracks)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 320\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(file)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:360\u001b[0m, in \u001b[0;36mMidiFile._load\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    356\u001b[0m     _dbg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeader:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m    359\u001b[0m  num_tracks,\n\u001b[0;32m--> 360\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticks_per_beat) \u001b[38;5;241m=\u001b[39m read_file_header(infile)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    363\u001b[0m     _dbg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-> type=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, tracks=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, ticks_per_beat=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype, num_tracks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticks_per_beat))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:106\u001b[0m, in \u001b[0;36mread_file_header\u001b[0;34m(infile)\u001b[0m\n\u001b[1;32m    103\u001b[0m name, size \u001b[38;5;241m=\u001b[39m read_chunk_header(infile)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMThd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMThd not found. Probably not a MIDI file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     data \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mread(size)\n",
      "\u001b[0;31mOSError\u001b[0m: MThd not found. Probably not a MIDI file"
     ]
    }
   ],
   "source": [
    "Dataset = PreProcessing(nDir = 2000)\n",
    "torch.save(Dataset, 'Dataset_Percussion1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2403bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bass  115561\n",
      "Piano  58756\n",
      "Guitar  185958\n",
      "Organ  44142\n",
      "Ensemble  62419\n",
      "Synth Lead  21766\n",
      "Reed  30971\n",
      "Sound Effects  8824\n",
      "Brass  33210\n",
      "Chromatic Percussion  29047\n",
      "Pipe  18956\n",
      "Percussion  6663\n",
      "Synth Pad  17083\n",
      "Strings  33716\n",
      "Percussive  6366\n",
      "Synth Effects  7385\n",
      "Ethnic  3040\n"
     ]
    }
   ],
   "source": [
    "#Dataset = torch.load('Dataset.pt', map_location='cpu')\n",
    "\n",
    "for key in Dataset.keys():\n",
    "   print(key, '', len(Dataset[key]))\n",
    "\n",
    "#Since we load it after, we free some space\n",
    "# del Dataset\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baf7636",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "Dataset['Bass']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb5535",
   "metadata": {},
   "source": [
    "# Monophonic Model and Architecture\n",
    "\n",
    "The class DatasetTransorm allow us to choose which intrument's bars to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09475244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are selecting the data from the dataset with the guitar instrument\n",
    "Data = DL.MonophonicDataset(Instrument='Guitar')\n",
    "trainData = DataLoader(Data, batch_size=10, shuffle=True, num_workers=0, collate_fn=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d495b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56575f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4e93f5",
   "metadata": {},
   "source": [
    "# Genre detection using CNN\n",
    "\n",
    "The idea is to train a Convolutional Neural Network (CNN) to understand the structure of the songs and to implement a classifier capable of identifying the genre of each song in our MIDI dataset.\n",
    "\n",
    "However, we cannot train the CNN directly on our MIDI dataset, since this would compromise both learning and classification. Moreover, CNNs are supervised learning models, and our dataset does not include genre labels. For this reason, we found another dataset containing 100 songs in .wav format for each of the following musical genres:\n",
    "\n",
    "- metal\n",
    "- disco\n",
    "- classical\n",
    "- hiphop\n",
    "- jazz\n",
    "- country\n",
    "- pop\n",
    "- blues \n",
    "- raggae \n",
    "- rock\n",
    "\n",
    "The idea is to train the CNN using this labeled dataset. Before doing that, we need to perform some preprocessing, since some songs in the dataset are corrupted. Additionally, the audio clips are only a few seconds long, so we preprocess each song to have a fixed length and a consistent format. The preprocessing functions are implemented in the file **CNN_ExtractGenre.py**.\n",
    "\n",
    "After preprocessing, we define the CNN model and the data loader in **Model.py** and **DataLoader.py**, respectively. The model is trained on Google Colab (not on the local machine), and we later load the trained model using its state_dict.\n",
    "\n",
    "The CNN achieves a strong validation accuracy of 84%, as shown in the accompanying paper.\n",
    "\n",
    "Once the model is trained on the labeled dataset, we use it to classify our own songs. This is a complex process because our songs are in .mid format, while the model expects .wav spectrograms as input. Therefore, each MIDI file must be converted into audio, transformed into a spectrogram, and then classified by the CNN.\n",
    "\n",
    "After classifying each song, we save a file containing the song’s name and its predicted genre. From there, we proceed as before: we separate our dataset by genre, and within each genre, we further separate the songs by instrument.\n",
    "\n",
    "\n",
    "N.B. all the function in the file **CNN_ExtractGenre** has already been runned since the computation is quite long. In the following cell we are showing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30a7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gordon Lightfoot/Sundown (9, 0.5)\n",
      "Gordon Lightfoot/Sundown.1 (9, 1.0)\n",
      "Gordon Lightfoot/Rainy Day People (9, 0.5)\n",
      "Gordon Lightfoot/Carefree Highway (9, 0.92)\n",
      "Gordon Lightfoot/Beautiful (4, 0.67)\n",
      "Gounod Charles/Ave Maria.1 (2, 0.92)\n",
      "Gounod Charles/Marche funebre d'une marionnette (2, 0.42)\n",
      "Gounod Charles/Waltz from Faust (2, 1.0)\n",
      "Grace Jones/Slave to the Rhythm (6, 0.58)\n",
      "Grand Funk Railroad/Some Kind of Wonderful (1, 1.0)\n",
      "Grand Funk Railroad/I'm Your Captain (Closer to Home) (9, 0.67)\n"
     ]
    }
   ],
   "source": [
    "#Load the preprocessed and classified dataset:\n",
    "with open('GenreDataset.pkl', 'rb') as f:\n",
    "   GenreDataset = pickle.load(f)\n",
    "\n",
    "#Mapping each genre into a number for classification\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "i = 0\n",
    "for key in GenreDataset.keys():\n",
    "   i += 1\n",
    "   print(key, GenreDataset[key])\n",
    "\n",
    "   if i > 10:\n",
    "      break\n",
    "#Author/Name of the song, (Genre, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e127",
   "metadata": {},
   "source": [
    "# Polyphonic Music Generator\n",
    "\n",
    "as now we are only considering monophonic tracks, we are thus losing all the information between notes of the same instrument and the correlation between instruments! Therefore we would like to try to implement a polyphonic music generator. \n",
    "\n",
    "The strategy is the same as before. Instead of having (128x16) matrix we have (4x128x16) where 4 is the maximum number of instruments that can be played at the same time. Now each matrix 128x16 does not encode for a single note as before, but it allows for multiple note of the same instrument.\n",
    "\n",
    "In the file **PolyphonicPreprocessing.py** there are all the function used to preprocess the clean_midi dataset and build the dataset of mapped songs, cathegorized by genre (using the genre recognition dataset built before). Here we are processing and storing the Polyphonic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecb3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   3%|▎         | 62/2000 [00:12<06:27,  5.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PolyphonicDataset \u001b[38;5;241m=\u001b[39m PolyphonicPreProcessing(nDir \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(PolyphonicDataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolyphonicDataset_Percussion.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Università/PhysicsOfData/Neural Network and Deep Learning/MusicGenerator_Project/PolyphonicPreprocessing.py:312\u001b[0m, in \u001b[0;36mPolyphonicPreProcessing\u001b[0;34m(nDir, Velocity)\u001b[0m\n\u001b[1;32m    309\u001b[0m FilePath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DirPath, file)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m#Cleaned monophonic: Some songs are corrupted:\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m mid \u001b[38;5;241m=\u001b[39m Func_CorruptedFile(FilePath, file, \u001b[38;5;28mdir\u001b[39m)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m    \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Università/PhysicsOfData/Neural Network and Deep Learning/MusicGenerator_Project/Preprocessing.py:71\u001b[0m, in \u001b[0;36mFunc_CorruptedFile\u001b[0;34m(FilePath, file, dir, LogFolder)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFunc_CorruptedFile\u001b[39m(FilePath, file, \u001b[38;5;28mdir\u001b[39m, LogFolder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFolder\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     70\u001b[0m    \u001b[38;5;28;01mtry\u001b[39;00m :\n\u001b[0;32m---> 71\u001b[0m       mid \u001b[38;5;241m=\u001b[39m mido\u001b[38;5;241m.\u001b[39mMidiFile(FilePath)\n\u001b[1;32m     72\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m mid\n\u001b[1;32m     74\u001b[0m    \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, KeySignatureError, \u001b[38;5;167;01mEOFError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:320\u001b[0m, in \u001b[0;36mMidiFile.__init__\u001b[0;34m(self, filename, file, type, ticks_per_beat, charset, debug, clip, tracks)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 320\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(file)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:371\u001b[0m, in \u001b[0;36mMidiFile._load\u001b[0;34m(self, infile)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    369\u001b[0m     _dbg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrack \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks\u001b[38;5;241m.\u001b[39mappend(read_track(infile,\n\u001b[1;32m    372\u001b[0m                               debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug,\n\u001b[1;32m    373\u001b[0m                               clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:218\u001b[0m, in \u001b[0;36mread_track\u001b[0;34m(infile, debug, clip)\u001b[0m\n\u001b[1;32m    216\u001b[0m     msg \u001b[38;5;241m=\u001b[39m read_sysex(infile, delta, clip)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     msg \u001b[38;5;241m=\u001b[39m read_message(infile, status_byte, peek_data, delta, clip)\n\u001b[1;32m    220\u001b[0m track\u001b[38;5;241m.\u001b[39mappend(msg)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/midifiles/midifiles.py:133\u001b[0m, in \u001b[0;36mread_message\u001b[0;34m(infile, status_byte, peek_data, delta, clip)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m byte \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m127\u001b[39m:\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata byte must be in range 0..127\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Message\u001b[38;5;241m.\u001b[39mfrom_bytes([status_byte] \u001b[38;5;241m+\u001b[39m data_bytes, time\u001b[38;5;241m=\u001b[39mdelta)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/messages/messages.py:163\u001b[0m, in \u001b[0;36mMessage.from_bytes\u001b[0;34m(cl, data, time)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse a byte encoded message.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mAccepts a byte string or any iterable of integers.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mThis is the reverse of msg.bytes() or msg.bin().\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m msg \u001b[38;5;241m=\u001b[39m cl\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(cl)\n\u001b[0;32m--> 163\u001b[0m msgdict \u001b[38;5;241m=\u001b[39m decode_message(data, time\u001b[38;5;241m=\u001b[39mtime)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m msgdict:\n\u001b[1;32m    165\u001b[0m     msgdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m SysexData(msgdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/messages/decode.py:103\u001b[0m, in \u001b[0;36mdecode_message\u001b[0;34m(msg_bytes, time, check)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid sysex end byte \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check:\n\u001b[0;32m--> 103\u001b[0m     check_data(data)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status_byte \u001b[38;5;129;01min\u001b[39;00m _SPECIAL_CASES:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status_byte \u001b[38;5;129;01min\u001b[39;00m CHANNEL_MESSAGES:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mido/messages/checks.py:44\u001b[0m, in \u001b[0;36mcheck_data\u001b[0;34m(data_bytes)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MIN_PITCHWHEEL \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pitch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m MAX_PITCHWHEEL:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpitchwheel value must be in range \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m..\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     41\u001b[0m                          MIN_PITCHWHEEL, MAX_PITCHWHEEL))\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_data\u001b[39m(data_bytes):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte \u001b[38;5;129;01min\u001b[39;00m data_bytes:\n\u001b[1;32m     46\u001b[0m         check_data_byte(byte)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PolyphonicDataset = PolyphonicPreProcessing(nDir = 2000)\n",
    "torch.save(PolyphonicDataset, 'PolyphonicDataset_Percussion.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddd389",
   "metadata": {},
   "source": [
    "And these are all the genre and the number of bars for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b133b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disco  14828\n",
      "country  9742\n",
      "rock  62342\n",
      "jazz  9811\n",
      "classical  9943\n",
      "pop  8046\n",
      "blues  1513\n",
      "raggae  3262\n",
      "hiphop  228\n",
      "metal  88\n"
     ]
    }
   ],
   "source": [
    "PolyphonicDataset = torch.load('PolyphonicDataset.pt', weights_only=False)\n",
    "\n",
    "for key in PolyphonicDataset:\n",
    "   print(key, '', len(PolyphonicDataset[key]))\n",
    "\n",
    "#We load with the DataLoader class, freeing some space\n",
    "# del PolyphonicDataset\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba03369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 2,  2,  2],\n",
       "                       [56, 61, 63],\n",
       "                       [ 6,  4,  0]]),\n",
       "       values=tensor([1, 1, 1]),\n",
       "       size=(4, 128, 16), nnz=3, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolyphonicDataset['disco'][0]['Bars'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18696ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(PolyphonicDataset['disco'][0]['Bars'][0].to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc14a19",
   "metadata": {},
   "source": [
    "And we can do the same thing done before saving the velocity information of the bars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a951e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [07:19<00:00,  4.55it/s] \n",
      "100%|██████████| 10/10 [00:00<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "PolyphonicDatasetVelocity = PolyphonicPreProcessing(nDir = 2000, Velocity=True)\n",
    "torch.save(PolyphonicDatasetVelocity, 'PolyphonicDatasetVelocity.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3901c4",
   "metadata": {},
   "source": [
    "And we can load the dataset using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452518b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolData = DL.PolyphonicDataset(Genre = 'jazz')\n",
    "PolTrainData = DataLoader(PolData, batch_size=30, shuffle=True, num_workers=0, collate_fn=cf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
