{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d021b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "from Preprocessing import *\n",
    "from CNN_ExtractGenre import *\n",
    "from PolyphonicPreprocessing import *\n",
    "\n",
    "import DatasetLoader as DL\n",
    "import Model as M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daceff",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Apply the functions in Preprocessing.py to clean the midi dataset. There are multiple files that are currupted or duplicated. \n",
    "For this analysis we are also going to use only Midi file with a timestamp of 4/4, like in the reference paper. This filtering is done in CleaningData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3973b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting Duplicates:: 100%|██████████| 2201/2201 [00:00<00:00, 2227.41it/s]\n",
      "100%|██████████| 2201/2201 [06:27<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "DeleteDuplicates()\n",
    "CleaningData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33b80b",
   "metadata": {},
   "source": [
    "The input of the model has to be a 128x16 matrix as in the paper. The following function clasify the midi tracks into instrumental classes:\n",
    "- Guitar  \n",
    "- Percussion\n",
    "- Organ  \n",
    "- Sound Effects \n",
    "- Bass  \n",
    "- Piano \n",
    "- Synth Lead \n",
    "- Chromatic Percussion \n",
    "- Synth Pad  \n",
    "- Percussive \n",
    "- Synth Effects\n",
    "- Ethnic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03fd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [07:57<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "MonophonicDataset = PreProcessing(nDir = 2000)\n",
    "torch.save(MonophonicDataset, 'MonophonicDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a64488",
   "metadata": {},
   "outputs": [],
   "source": [
    "MonophonicDataset = torch.load('MonophonicDataset.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2403bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bass                      number of Bars: 106314\n",
      "Ensemble                  number of Bars: 53067\n",
      "Organ                     number of Bars: 37496\n",
      "Guitar                    number of Bars: 176320\n",
      "Piano                     number of Bars: 48277\n",
      "Synth Lead                number of Bars: 18727\n",
      "Percussive                number of Bars: 4689\n",
      "Synth Effects             number of Bars: 6506\n",
      "Reed                      number of Bars: 28849\n",
      "Brass                     number of Bars: 31106\n",
      "Synth Pad                 number of Bars: 13475\n",
      "Pipe                      number of Bars: 16226\n",
      "Strings                   number of Bars: 27713\n",
      "Sound Effects             number of Bars: 7644\n",
      "Chromatic Percussion      number of Bars: 25030\n",
      "Percussion                number of Bars: 3890\n",
      "Ethnic                    number of Bars: 2970\n"
     ]
    }
   ],
   "source": [
    "for key in MonophonicDataset.keys():\n",
    "   print(f\"{key.ljust(25)} number of Bars: {len(MonophonicDataset[key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857bf53",
   "metadata": {},
   "source": [
    "### N.B.\n",
    "\n",
    "in order to save disk memory the 128x16 matrix are stored as sparse matrix since the majority of the element of the matricies are zeros.\n",
    "In the next cell we are showing how the dataset is done: the dataset is diveded into the instruments classes and each class is a list of bars. Each list contains the SongName, a tuple containing ($Bar_{i-1}$, $Bar_{i}$), the Program (which correspond to the instrument played, see **Util.py**), the Channel of the song (superfluous), the number of the bar (superfluous) and the Tempo of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74096a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SongName': (\"It's a Real Good Feeling\", \"It's a Real Good Feeling\"),\n",
       " 'Bars': (tensor(indices=tensor([[29, 29, 29, 29, 29, 29, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "                          38, 38, 38, 38],\n",
       "                         [ 0,  1,  2,  3,  4,  5,  5,  6,  7, 11, 12, 13, 14, 15,\n",
       "                           8,  9, 10, 11]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "         size=(128, 16), nnz=18, dtype=torch.int32, layout=torch.sparse_coo),\n",
       "  tensor(indices=tensor([[34, 34, 34, 34, 34, 34, 41, 41, 41, 41, 41, 41, 41, 43,\n",
       "                          43, 43, 43],\n",
       "                         [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 12, 13, 14, 15,  8,\n",
       "                           9, 10, 11]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "         size=(128, 16), nnz=17, dtype=torch.int32, layout=torch.sparse_coo)),\n",
       " 'Program': 35,\n",
       " 'Channel': (1, 1),\n",
       " 'numBar': (2, 3),\n",
       " 'Tempo': (123, 123)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First element of the Bass instruments. There are all the possible information needed\n",
    "MonophonicDataset['Bass'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125542de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we load it after, we free some space\n",
    "del MonophonicDataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e93f5",
   "metadata": {},
   "source": [
    "# Genre detection using CNN\n",
    "\n",
    "The idea is to train a Convolutional Neural Network (CNN) to understand the structure of the songs and to implement a classifier capable of identifying the genre of each song in our MIDI dataset.\n",
    "\n",
    "However, we cannot train the CNN directly on our MIDI dataset, since this would compromise both learning and classification. Moreover, CNNs are supervised learning models, and our dataset does not include genre labels. For this reason, we found another dataset containing 100 songs in .wav format for each of the following musical genres:\n",
    "\n",
    "- metal\n",
    "- disco\n",
    "- classical\n",
    "- hiphop\n",
    "- jazz\n",
    "- country\n",
    "- pop\n",
    "- blues \n",
    "- raggae \n",
    "- rock\n",
    "\n",
    "The idea is to train the CNN using this labeled dataset. Before doing that, we need to perform some preprocessing, since some songs in the dataset are corrupted. Additionally, the audio clips are only a few seconds long, so we preprocess each song to have a fixed length and a consistent format. The preprocessing functions are implemented in the file **CNN_ExtractGenre.py**.\n",
    "\n",
    "After preprocessing, we define the CNN model and the data loader in **Model.py** and **DataLoader.py**, respectively. The model is trained on Google Colab (not on the local machine), and we later load the trained model using its state_dict.\n",
    "\n",
    "The CNN achieves a strong validation accuracy of 84%, as shown in the accompanying paper.\n",
    "\n",
    "Once the model is trained on the labeled dataset, we use it to classify our own songs. This is a complex process because our songs are in .mid format, while the model expects .wav spectrograms as input. Therefore, each MIDI file must be converted into audio, transformed into a spectrogram, and then classified by the CNN.\n",
    "\n",
    "\n",
    "N.B. all the function in the file **CNN_ExtractGenre** has already been runned since the computation is quite long. In the following cell we are showing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c30a7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song name: Gordon Lightfoot/Sundown                                     Genre and probability:    (9, 0.50)\n",
      "Song name: Gordon Lightfoot/Sundown.1                                   Genre and probability:    (9, 1.00)\n",
      "Song name: Gordon Lightfoot/Rainy Day People                            Genre and probability:    (9, 0.50)\n",
      "Song name: Gordon Lightfoot/Carefree Highway                            Genre and probability:    (9, 0.92)\n",
      "Song name: Gordon Lightfoot/Beautiful                                   Genre and probability:    (4, 0.67)\n",
      "Song name: Gounod Charles/Ave Maria.1                                   Genre and probability:    (2, 0.92)\n",
      "Song name: Gounod Charles/Marche funebre d'une marionnette              Genre and probability:    (2, 0.42)\n",
      "Song name: Gounod Charles/Waltz from Faust                              Genre and probability:    (2, 1.00)\n",
      "Song name: Grace Jones/Slave to the Rhythm                              Genre and probability:    (6, 0.58)\n",
      "Song name: Grand Funk Railroad/Some Kind of Wonderful                   Genre and probability:    (1, 1.00)\n",
      "Song name: Grand Funk Railroad/I'm Your Captain (Closer to Home)        Genre and probability:    (9, 0.67)\n"
     ]
    }
   ],
   "source": [
    "#Load the preprocessed and classified dataset:\n",
    "with open('GenreDataset.pkl', 'rb') as f:\n",
    "   GenreDataset = pickle.load(f)\n",
    "\n",
    "#Mapping each genre into a number for classification\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "i = 0\n",
    "for key in GenreDataset.keys():\n",
    "   i += 1\n",
    "   genre, prob = GenreDataset[key]\n",
    "   print(f\"{'Song name:':<10} {key.ljust(60)} {'Genre and probability:':<25} ({genre}, {prob:.2f})\")\n",
    "\n",
    "   if i > 10:\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e127",
   "metadata": {},
   "source": [
    "# Polyphonic Music Generator\n",
    "\n",
    "as now we are only considering monophonic tracks, we are thus losing all the information between notes of the same instrument and the correlation between instruments! Therefore we would like to try to implement a polyphonic music generator. \n",
    "\n",
    "The strategy is the same as before. Instead of having (128x16) matrix we have (4x128x16) where 4 is the maximum number of instruments that can be played at the same time. Now each matrix 128x16 does not encode for a single note as before, but it allows for multiple note of the same instrument.\n",
    "\n",
    "In the file **PolyphonicPreprocessing.py** there are all the function used to preprocess the clean_midi dataset and build the dataset of mapped songs. Here we are processing and storing the Polyphonic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecb3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [06:14<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3143\n"
     ]
    }
   ],
   "source": [
    "PolyphonicDataset = PolyphonicPreProcessing(nDir = 2000)\n",
    "torch.save(PolyphonicDataset, 'PolyphonicDataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddd389",
   "metadata": {},
   "source": [
    "And these are all the genre and the number of bars for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "294b81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolyphonicDataset = torch.load('PolyphonicDataset.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29803610",
   "metadata": {},
   "source": [
    "As before the 4x128x16 matrix is stored as a sparse matrix to save space. There is also the informationa about the genre of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9dffc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SongName': ('The Righteous Brothers/Unchained Melody.3',\n",
       "  'The Righteous Brothers/Unchained Melody.3'),\n",
       " 'Bars': (tensor(indices=tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "                           0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1],\n",
       "                         [31, 33, 33, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "                          36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 37, 37,\n",
       "                          42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "                          42, 42],\n",
       "                         [ 6,  6,  7,  0,  1,  2,  3,  4,  5,  8,  9, 10, 11, 12,\n",
       "                          13, 14,  0,  1,  6,  7,  8,  9, 14, 15,  4,  5, 12, 13,\n",
       "                           0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                          14, 15]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1]),\n",
       "         size=(4, 128, 16), nnz=44, layout=torch.sparse_coo),\n",
       "  tensor(indices=tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "                           0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "                           1,  1],\n",
       "                         [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "                          33, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 37, 37, 42,\n",
       "                          42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
       "                          42, 46],\n",
       "                         [ 0,  1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14,\n",
       "                          15,  0,  1,  6,  7,  8,  9, 14, 15,  4,  5, 12, 13,  0,\n",
       "                           1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "                          15, 15]]),\n",
       "         values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                        1, 1, 1, 1, 1, 1]),\n",
       "         size=(4, 128, 16), nnz=44, layout=torch.sparse_coo)),\n",
       " 'Program': [32, 150, 48, 48],\n",
       " 'ActiveProgram': (array([1, 1, 0, 0]), array([1, 1, 0, 0])),\n",
       " 'numBar': (2, 3),\n",
       " 'Tempo': 80,\n",
       " 'Genre': 9}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolyphonicDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e19551",
   "metadata": {},
   "outputs": [],
   "source": [
    "del PolyphonicDataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d793b2",
   "metadata": {},
   "source": [
    "# Monophonic and Polyphonic Model and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab54707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose if the training is done with the Monophonic Dataset or Polyphonic Dataset\n",
    "Mono = False\n",
    "\n",
    "EPOCHS = 20\n",
    "noise_dim = 100\n",
    "BATCH_SIZE = 70\n",
    "\n",
    "if Mono:\n",
    "                              #Train the network on these instruments\n",
    "  Data = DL.MonophonicDataset(Instruments=['Piano', 'Guitar', 'Bass'])\n",
    "  dataloader = DataLoader(Data, BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "else: Data = DL.PolyphonicDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "253452ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "   print('GPU available')\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device('mps')\n",
    "   print('mps available')\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "   print('GPU not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438fa2c",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "Definition of the concatenation functions that are used in the hidden layers to concatenate the output and the 1_d and 2_d conditions.\n",
    "\n",
    "1_d conditioning vector of shape $[n,1]$ with an output of shape $[batch,features,a,b]$:\n",
    "* first we have to duplicate the vector $a\\cdot b$ times to get a tensor of shape $[batch,n,a,b]$\n",
    "* then we can concatenate the two tensors in the depth dimension (i.e dim=1)\n",
    "\n",
    "2_d conditioning matrix of the same shape of the output $[batch,features,a,b]$ except the depth dim (it must be that because how we build the conditioner CNN):\n",
    "* first we check that the dimensions are correct\n",
    "* we concatenate the two tensors in the depth dimension (i.e dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf8acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_cond_concat(x, y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "    x_shapes = x.shape  #[batch,n_features,a,b]\n",
    "    y_shapes = y.shape  #[batch,n]\n",
    "    y2 = y.view(x_shapes[0],y_shapes[1],1,1)                              #[batch,n,1,1]\n",
    "    y2 = y2.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])      #[batch,n,a,b]\n",
    "\n",
    "    return torch.cat((x, y2),dim=1)                                     #[batch,n_features+n,a,b]\n",
    "\n",
    "def conv_prev_concat(x, y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "    x_shapes = x.shape  #[batch,n_features,a,b]\n",
    "    y_shapes = y.shape  #[batch,16,a,b]\n",
    "    if x_shapes[2:] == y_shapes[2:]:\n",
    "        y2 = y.expand(x_shapes[0],y_shapes[1],x_shapes[2],x_shapes[3])  #[batch,16,a,b]\n",
    "\n",
    "        return torch.cat((x, y2),dim=1)                                 #[batch,n_features+16,a,b]\n",
    "\n",
    "    else:\n",
    "        print(x_shapes[2:])\n",
    "        print(y_shapes[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7fa53",
   "metadata": {},
   "source": [
    "### The Generator and the Conditioner\n",
    "\n",
    "The generator uses `ConvTranspose2d` (upsampling) layers to produce an image from a seed (random noise). Start with two `Dense` layers that take this seed as input and transform it to a tensor of shape $[\\text{batch size}, 128, 1, 2]$, then upsample several times until we reach the desired size of a bar of $[\\text{instrument}, 128, 16]$. We use  the `ReLU` activation for each layer, except the output layer which can use `Sigmoid` to predict pixel values in the [0, 1] range.\n",
    "\n",
    "Coupled to the generator there is the conditioner that uses `Conv2d` (sampling) layers to produce the 2_d tensors that serve as informations from the preaviou bar. The conditioner can be viewed as the reverse of the generator because it uses filters with the same shapes of the ones in the generator. In this case we use  the `LeakyReLU` activation for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d659c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "   def __init__(self, input_size, cond_1d_size, instrument_size=1, n_hlayers=128):\n",
    "      super().__init__()\n",
    "\n",
    "      self.input_size = input_size\n",
    "      self.instrument_size = instrument_size\n",
    "      self.cond1d_dim = cond_1d_size\n",
    "\n",
    "      #generator layers\n",
    "      #as said in the DCGAN paper always ReLU activation function in the generator excluded the last layer\n",
    "      #as said in the DCGAN paper always batchnorm iin the generator excluded the last layer\n",
    "      self.ff1 = nn.Sequential(\n",
    "         nn.Linear(input_size+cond_1d_size, 1024),                                                                                     #[batch,1024]\n",
    "         nn.BatchNorm1d(1024),\n",
    "         nn.ReLU()\n",
    "         )\n",
    "      self.ff2 = nn.Sequential(\n",
    "         nn.Linear(1024+cond_1d_size,n_hlayers*2),                                                                                    #[batch,512]\n",
    "         nn.BatchNorm1d(n_hlayers*2),\n",
    "         nn.ReLU()\n",
    "         )\n",
    "      #reshape to [batch size,128,1,2]\n",
    "      # #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "      self.cnn1 = nn.Sequential(\n",
    "         nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, n_hlayers, kernel_size=(1,2), stride=(2,2), bias=False, padding=0),           #[batch,128,1,4]\n",
    "         nn.BatchNorm2d(n_hlayers),\n",
    "         nn.ReLU()\n",
    "         )\n",
    "      #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "      self.cnn2 = nn.Sequential(\n",
    "         nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, n_hlayers, kernel_size=(1,2), stride=(2,2), bias=False, padding=0),           #[batch,128,1,8]\n",
    "         nn.BatchNorm2d(n_hlayers),\n",
    "         nn.ReLU()\n",
    "         )\n",
    "      #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "      self.cnn3 = nn.Sequential(\n",
    "         nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, n_hlayers, kernel_size=(1,2), stride=(2,2), bias=False, padding=0),           #[batch,128,1,16]\n",
    "         nn.BatchNorm2d(n_hlayers),\n",
    "         nn.ReLU()\n",
    "         )\n",
    "      #+condition [batch,128+cond_1d_size+16,1,2]\n",
    "      self.cnn4 = nn.Sequential(\n",
    "         nn.ConvTranspose2d(n_hlayers+cond_1d_size+16, instrument_size, \n",
    "                           kernel_size=(128,1), stride=(2,1), bias=False, padding=0),                                      #[batch,instrument_size,128,16]\n",
    "         nn.Sigmoid()\n",
    "         #Sigmoid funciotn because we want to generate the matrixes of music without velocity, i.e. only (0,1)\n",
    "         #Thus we use the sigmoid which is a smoother version of the sign function\n",
    "         )\n",
    "      #conditioner layers\n",
    "      # #as in Midinet model we use the Leaky activation funciton for the conditioner\n",
    "      self.h0_prev = nn.Sequential(\n",
    "         nn.Conv2d(in_channels=instrument_size, out_channels=16, kernel_size=(128,1), stride=(2,1)),                  #[batch,16,1,16]\n",
    "         nn.BatchNorm2d(16),\n",
    "         nn.LeakyReLU()          #note: in the original paper leak=0.2, default leak=0.01\n",
    "         )\n",
    "      self.h1_prev = nn.Sequential(\n",
    "         nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,2), stride=(2,2)),                                  #[batch,16,1,8]\n",
    "         nn.BatchNorm2d(16),\n",
    "         nn.LeakyReLU()\n",
    "         )\n",
    "      self.h2_prev = nn.Sequential(\n",
    "         nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,2), stride=(2,2)),                                  #[batch,16,1,4]\n",
    "         nn.BatchNorm2d(16),\n",
    "         nn.LeakyReLU()\n",
    "         )\n",
    "      self.h3_prev = nn.Sequential(\n",
    "         nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1,2), stride=(2,2)),                                  #[batch,16,1,2]\n",
    "         nn.BatchNorm2d(16),\n",
    "         nn.LeakyReLU()\n",
    "         )\n",
    "\n",
    "   def forward(self, z, prev_bar, cond_1d, batch_size):\n",
    "\n",
    "      #2d condiiton\n",
    "      cond0 = self.h0_prev(prev_bar)          #[batch,16,1,16]\n",
    "      cond1 = self.h1_prev(cond0)             #[batch,16,1,8]\n",
    "      cond2 = self.h2_prev(cond1)             #[batch,16,1,4]\n",
    "      cond3 = self.h3_prev(cond2)             #[batch,16,1,2]\n",
    "\n",
    "      #single cond_1d size =[n,1], batch_cond_1d size = [batch_size,n]\n",
    "\n",
    "      input = torch.cat((z,cond_1d), dim=1)   #[batch_size, input_size+cond_1d_size]\n",
    "\n",
    "      h0 = self.ff1(input)                    #[batch,1024]\n",
    "      h0 = torch.cat((h0,cond_1d), dim=1)     #[batch,1024+cond_1d_size]\n",
    "\n",
    "      h1 = self.ff2(h0)                       #[batch,256]\n",
    "      h1 = h1.reshape(batch_size, 128, 1, 2)  #[batch,128,1,2]\n",
    "      h1 = conv_cond_concat(h1,cond_1d)       #[batch,128+cond_1d_size,1,2]\n",
    "      h1 = conv_prev_concat(h1,cond3)         #[batch,128+cond_1d_size+16,1,2]\n",
    "\n",
    "      h2 = self.cnn1(h1)                      #[batch,128,1,4]\n",
    "      h2 = conv_cond_concat(h2,cond_1d)       #[batch,128+cond_1d_size,1,4]\n",
    "      h2 = conv_prev_concat(h2,cond2)         #[batch,128+cond_1d_size+16,1,4]\n",
    "\n",
    "      h3 = self.cnn2(h2)                      #[batch,128,1,8]\n",
    "      h3 = conv_cond_concat(h3,cond_1d)       #[batch,128+cond_1d_size,1,8]\n",
    "      h3 = conv_prev_concat(h3,cond1)         #[batch,128+cond_1d_size+16,1,8]\n",
    "\n",
    "      h4 = self.cnn3(h3)                      #[batch,128,1,16]\n",
    "      h4 = conv_cond_concat(h4,cond_1d)       #[batch,128+cond_1d_size,1,16]\n",
    "      h4 = conv_prev_concat(h4,cond0)         #[batch,128+cond_1d_size+16,1,16]\n",
    "\n",
    "      out = self.cnn4(h4)                     #[batch,instrument_size,128,16]\n",
    "\n",
    "      return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6fc97",
   "metadata": {},
   "source": [
    "### The Discriminator\n",
    "\n",
    "The discriminator uses `Conv2d` (sampling) layers to produce a scalar output from a bar input. Start with two `Conv2d` layers that reduce the size of the input, then use two `Dense` layers. We use  the `LeakyReLU` activation for each layer, except the output layer which can use `Sigmoid` to predict true-false probability value in the [0, 1] range. Note that the activation is included in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f6a37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscrimination(nn.Module):\n",
    "    def __init__(self, in_features, out_features, kernel_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.T = nn.Parameter(torch.randn(in_features, out_features, kernel_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is batch_sizexA\n",
    "        # T is AxBxC\n",
    "        matrices = x @ self.T.view(self.in_features, -1)                     # matrix moltiplication, result shape: [batch_size, B*C]\n",
    "        matrices = matrices.view(-1, self.out_features, self.kernel_dim)     #M shape [batch, B, C]\n",
    "\n",
    "        # compute L1 distances between samples\n",
    "        M = matrices.unsqueeze(0)               # [1,batch,B,C]\n",
    "        M_T = M.permute(1, 0, 2, 3)             # [batch,1,B,C]\n",
    "        norm = torch.abs(M - M_T).sum(3)        # first broadcast [batch,batch,B,C], then [batch,batch,B]\n",
    "        cbij = torch.exp(-norm)\n",
    "        o_b = cbij.sum(0)                       # [batch,B], if j !=0 i in teh sum then subtract self distance (cbij.sum(0) - 1)\n",
    "\n",
    "        x = torch.cat([x, o_b], 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d4bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, cond_1d_size, instrument_size=1, mini_size=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.instrument_size = instrument_size\n",
    "        self.cond1d_dim = cond_1d_size\n",
    "\n",
    "        #as said in the DCGAN paper always batchnorm in the discriminator layers excluded the first layer\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(2*instrument_size+cond_1d_size, 32, kernel_size=(128,2), stride=(2,2), padding=0),        #[batch,32,1,8]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        #+condition [batch,14+cond_1d_size,1,8]\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(32+cond_1d_size, 77, kernel_size=(1,4), stride=2, padding=0),                             #[batch,77,1,3]\n",
    "            #Adding residual block\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.ffnn1 = nn.Sequential(\n",
    "             #+condition [batch,231+cond_1d_size]\n",
    "            nn.Linear(231+cond_1d_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.minibatch = MinibatchDiscrimination(in_features=1024, out_features=mini_size,kernel_dim=3)\n",
    "\n",
    "        #+condition [batch,1024+mini_size+cond_1d_size]\n",
    "        #no sigmoid activation function because it is already in the definition of the cross entropy loss function\n",
    "        self.ffnn2 = nn.Linear(1024+cond_1d_size+mini_size, 1)      \n",
    "\n",
    "\n",
    "    def forward(self, x, prev_bar, cond_1d):\n",
    "        input = conv_cond_concat(x,cond_1d)         #[batch,instrument_size+cond_1d_size,128,16]\n",
    "        input = conv_prev_concat(input,prev_bar)    #[batch,2*instrument_size+cond_1d_size,128,16]\n",
    "\n",
    "        h0 = self.cnn1(input)                       #[batch,14,1,8]\n",
    "        fm = h0\n",
    "        h0 = conv_cond_concat(h0, cond_1d)          #[batch,14+cond_1d_size,1,8]\n",
    "\n",
    "        h1 = self.cnn2(h0)                          #[batch,77,1,3]\n",
    "        h1 = torch.flatten(h1, 1)                   #[batch,77*3*1]\n",
    "        h1 = torch.cat((h1,cond_1d),dim=1)          #[batch,231+cond_1d_size]\n",
    "\n",
    "        h2 = self.ffnn1(h1)                         #[batch,1024]\n",
    "        h2 = self.minibatch(h2)                     #[batch,1024+mini_size]\n",
    "        h2 = torch.cat((h2,cond_1d),dim=1)          #[batch,1024+mini_size+cond_1d_size]\n",
    "\n",
    "        h3 = self.ffnn2(h2)                         #[batch,1]\n",
    "        h3_sigmoid = torch.sigmoid(h3)\n",
    "\n",
    "\n",
    "        return h3_sigmoid, h3, fm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6c775",
   "metadata": {},
   "source": [
    "### Weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2726b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.ConvTranspose2d, nn.Conv2d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  # DCGAN standard\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4b952",
   "metadata": {},
   "source": [
    "### Creation of Generator and Discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3f3cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (cnn1): Sequential(\n",
       "    (0): Conv2d(14, 32, kernel_size=(128, 2), stride=(2, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (cnn2): Sequential(\n",
       "    (0): Conv2d(38, 77, kernel_size=(1, 4), stride=(2, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (ffnn1): Sequential(\n",
       "    (0): Linear(in_features=237, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (minibatch): MinibatchDiscrimination()\n",
       "  (ffnn2): Linear(in_features=1130, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Mono:\n",
    "  generator = Generator(input_size=100, cond_1d_size=2, instrument_size=1, n_hlayers=128)\n",
    "  discriminator = Discriminator(cond_1d_size=2, instrument_size=1)\n",
    "else:\n",
    "  generator = Generator(input_size=100, cond_1d_size=6, instrument_size=4, n_hlayers=128)\n",
    "  discriminator = Discriminator(cond_1d_size=6, instrument_size=4)\n",
    "\n",
    "\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9ecb9",
   "metadata": {},
   "source": [
    "### Dimension testing\n",
    "\n",
    "Produce a noise vector of size `[10, 100]`, a noise 1d condition vector of size `[10, 15]`, and a noise 2d condition tensor of size `[10, 1, 128,16]`. Note that we need a 1d and a 2d contions for each batch input. Then we use the (as yet **untrained**) generator to create an image of expected output shape $[10,1,128,16]$.\n",
    "\n",
    "Then use the (yet **untrained**) discriminator to classify the generated images as real or fake. The model will be trained to output the probability that the image is real in the first output component, thus we expect an output vector of size `[10, 1]` with $x_i \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a39486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n",
      "torch.Size([10, 4, 128, 16])\n",
      "tensor([[0.3348],\n",
      "        [0.3703],\n",
      "        [0.7252],\n",
      "        [0.4951],\n",
      "        [0.5680],\n",
      "        [0.5459],\n",
      "        [0.6740],\n",
      "        [0.5166],\n",
      "        [0.6564],\n",
      "        [0.8628]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "############################ input (batch_size=10, vector_size=100) ###############################\n",
    "noise = torch.normal(0, 1, [10, 100])\n",
    "print(noise.shape)\n",
    "############################ conditions ###############################\n",
    "if Mono:\n",
    "  cond_1d = torch.normal(0,1,[10,2])\n",
    "  prev_bar = torch.normal(0, 1, [10, 1, 128, 16])\n",
    "else:\n",
    "  cond_1d = torch.normal(0,1,[10,6])\n",
    "  prev_bar = torch.normal(0, 1, [10, 4, 128, 16])\n",
    "\n",
    "\n",
    "############################ generator ###############################\n",
    "generated_bar = generator(noise, prev_bar, cond_1d, batch_size=10).detach()\n",
    "print(generated_bar.shape)\n",
    "############################ discriminator ###############################\n",
    "decision, __, __= discriminator(generated_bar, prev_bar, cond_1d)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c605e",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s.\n",
    "The discriminator loss is of the form:\n",
    "\n",
    "$\\frac{1}{m}\\sum_{i=1}^{m}-[\\log D(\\boldsymbol{x}^{(i)}) +\\log(1-D(G(\\boldsymbol{z}^{(i)})))]$\n",
    "\n",
    "We inplement one-sided label smoothing to penalize self confidence and imporve the convergence of the training. Thus we substitute the discriminator's predictions on real images to an array of 1s with an array of (1-$\\alpha$)s and the loss function becomes:\n",
    "\n",
    "$\\frac{1}{m}\\sum_{i=1}^{m}-[(1-\\alpha) \\log D(\\boldsymbol{x}^{(i)}) +\\alpha \\log (1-D(\\boldsymbol{x}^{(i)}))+\\log(1-D(G(\\boldsymbol{z}^{(i)})))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe6c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.BCEWithLogitsLoss()\n",
    "MSE=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf80ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, device, alpha=0.1):\n",
    "\n",
    "   #real_targets = torch.ones_like(real_output, device=device)                 #no label smoothing -> True output expected output is 1\n",
    "   real_targets = torch.full_like(real_output, 1.0 - alpha, device=device)     #one side label smoothing to penalize self confidence\n",
    "   fake_targets = torch.zeros_like(fake_output, device=device)                 #no label smoothing -> Fake output expected output is 0\n",
    "\n",
    "   real_loss = cross_entropy(real_output, real_targets)\n",
    "   fake_loss = cross_entropy(fake_output, fake_targets)\n",
    "\n",
    "   total_loss = real_loss + fake_loss\n",
    "   return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683f928",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).\n",
    "The generator loss is of the form:\n",
    "\n",
    "$\\frac{1}{m}\\sum_{i=1}^{m}\\log(1-D(G(\\boldsymbol{z}^{(i)})))$\n",
    "\n",
    "However this loss has some convergence issues due to vanishing gradients. So instead we use the following loss which has the same trend but stronger gradient when the discriminator is too good at recognizing fake samples.\n",
    "\n",
    "$\\frac{1}{m}\\sum_{i=1}^{m}-\\log(D(G(\\boldsymbol{z}^{(i)})))$\n",
    "\n",
    "Which is the Binary crossentropy between $D(G(\\boldsymbol{z}^{(i)}))$ and the probability distribution that has $y^{(i)} = 1 \\forall i$, i.e. we are forcing the generator to produce samples that will make the discriminator predict that fake samples are real.\n",
    "\n",
    "Moreover we add a regularizer term so-called feature matching such that the distributions of the real and generated data are enforced to be close.\n",
    "\n",
    "$\\lambda_1 ||E_{x \\sim p(x)} [x] - E_{z\\sim p(z)} [G(z)] ||^2 + \\lambda_2 ||E_{x \\sim p(x)} [f(x)] - E_{z \\sim p(z)} [f(G(z))] ||^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d57f76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, real_bar, fake_bar, real_f, fake_f, device, lambda1=0.06, lambda2=0.006):\n",
    "\n",
    "   gen_loss = cross_entropy(fake_output, torch.ones_like(fake_output, device=device))\n",
    "\n",
    "   mean_real = torch.mean(real_bar, dim=0)\n",
    "   mean_fake = torch.mean(fake_bar, dim=0)\n",
    "   l2_data = MSE(mean_real, mean_fake)\n",
    "\n",
    "   mean_real_feat = torch.mean(real_f, dim=0)\n",
    "   mean_fake_feat = torch.mean(fake_f, dim=0)\n",
    "   l2_feat = MSE(mean_real_feat, mean_fake_feat)\n",
    "\n",
    "   return gen_loss+lambda1*l2_data+lambda2*l2_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8dca6",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "With DCGAN the training is very diffuclt so we decide to use Adam optimizer as suggested by the paper. Note that with Adam we use both momentum and RMSprop to normalized velocities. Discriminator and generator need two different optimizers (conditioner is included in the generator training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f51155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "gen_opt = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "dis_opt = optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c884d2",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "502f7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(bars, cond_1d, prev_bar, generator, discriminator, batch_size, noise_dim, device, dis_opt, gen_opt, epoch):\n",
    "  # --- Ensure all tensors are on the correct device ---\n",
    "  bars = bars.to(device)\n",
    "  cond_1d = cond_1d.to(device)\n",
    "  prev_bar = prev_bar.to(device)\n",
    "\n",
    "  # --- Discriminator training ---\n",
    "  noise = torch.randn([batch_size, noise_dim], device=device)\n",
    "\n",
    "  bars_noise = bars + torch.randn_like(bars) * 0.1\n",
    "  bars_noise = torch.clamp(bars_noise, 0, 1)\n",
    "\n",
    "  # Generate fake samples\n",
    "  generated_bars = generator(noise, prev_bar, cond_1d, batch_size)\n",
    "\n",
    "  # Forward pass (real + fake)\n",
    "  _, real_D, _ = discriminator(bars_noise, prev_bar, cond_1d)\n",
    "  _, fake_D, _ = discriminator(generated_bars.detach(), prev_bar, cond_1d)\n",
    "\n",
    "  # Discriminator loss\n",
    "  disc_loss = discriminator_loss(real_D, fake_D, device)\n",
    "  discriminator.zero_grad()\n",
    "  disc_loss.backward()\n",
    "  dis_opt.step()\n",
    "\n",
    "  # --- Generator training (2 steps) ---\n",
    "  gen_losses = []\n",
    "  #We let the generator train more than the discriminator\n",
    "  if epoch > 4:\n",
    "    for _ in range(3):  \n",
    "      noise = torch.randn([batch_size, noise_dim], device=device)\n",
    "      generated_bars = generator(noise, prev_bar, cond_1d, batch_size)\n",
    "      _, fake_D, fake_fm = discriminator(generated_bars, prev_bar, cond_1d)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          _, real_D, real_fm = discriminator(bars_noise, prev_bar, cond_1d)\n",
    "\n",
    "      gen_loss = generator_loss(fake_D, bars, generated_bars, real_fm, fake_fm, device)\n",
    "\n",
    "      generator.zero_grad()\n",
    "      gen_loss.backward()\n",
    "      gen_opt.step()\n",
    "      gen_losses.append(gen_loss.item())\n",
    "\n",
    "  else:\n",
    "    for _ in range(1): \n",
    "      noise = torch.randn([batch_size, noise_dim], device=device)\n",
    "      generated_bars = generator(noise, prev_bar, cond_1d, batch_size)\n",
    "      _, fake_D, fake_fm = discriminator(generated_bars, prev_bar, cond_1d)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          _, real_D, real_fm = discriminator(bars_noise, prev_bar, cond_1d)\n",
    "\n",
    "      gen_loss = generator_loss(fake_D, bars, generated_bars, real_fm, fake_fm, device)\n",
    "\n",
    "      generator.zero_grad()\n",
    "      gen_loss.backward()\n",
    "      gen_opt.step()\n",
    "      gen_losses.append(gen_loss.item())\n",
    "\n",
    "  return sum(gen_losses) / len(gen_losses), disc_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91efa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TraningGAN():\n",
    "   gloss = []\n",
    "   dloss = []\n",
    "\n",
    "   for epoch in range(EPOCHS):\n",
    "\n",
    "      generator.train()\n",
    "      discriminator.train()\n",
    "      gen_losses = []\n",
    "      disc_losses = []\n",
    "      print('#################')\n",
    "      print(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "\n",
    "      if not Mono: dataloader = DL.getDataloader(Data, BATCH_SIZE, num_batches = 400)\n",
    "      iterator = tqdm(dataloader)\n",
    "      for bar_batch, prev_bar_batch, instrument_batch in iterator:\n",
    "         bar_batch = bar_batch.to(dtype=torch.float32, device=device)\n",
    "         prev_bar_batch = prev_bar_batch.to(dtype=torch.float32, device=device)\n",
    "         instrument_batch = instrument_batch.to(dtype=torch.float32, device=device)\n",
    "\n",
    "         if Mono:\n",
    "            bar_batch = bar_batch.unsqueeze(1)\n",
    "            prev_bar_batch=prev_bar_batch.unsqueeze(1)\n",
    "\n",
    "         gen_loss, disc_loss = train_step(bar_batch, instrument_batch, prev_bar_batch, generator, discriminator,\n",
    "                                          BATCH_SIZE, noise_dim, device, dis_opt, gen_opt, epoch)\n",
    "         gen_losses.append(gen_loss)\n",
    "         disc_losses.append(disc_loss)\n",
    "\n",
    "         iterator.set_description('Discriminator loss: {}, Generator loss: {}'.format(disc_loss, gen_loss))\n",
    "\n",
    "      gloss.append(np.mean(gen_losses))\n",
    "      dloss.append(np.mean(disc_losses))\n",
    "      #print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "      print(f'dLoss: {dloss[-1]}, gLoss: {gloss[-1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1279dc9",
   "metadata": {},
   "source": [
    "### Traning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "TraningGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discriminator.state_dict(), 'DiscParam.torch')\n",
    "torch.save(generator.state_dict(), 'GenParam.torch')\n",
    "print('Saved Model')\n",
    "\n",
    "torch.save(dis_opt.state_dict(), 'DiscStateParam.torch')\n",
    "torch.save(gen_opt.state_dict(), 'GenStateParam.torch')\n",
    "print('Saved Optimizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
