{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d021b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from Preprocessing import *\n",
    "from CNN_ExtractGenre import *\n",
    "from PolyphonicPreprocessing import *\n",
    "\n",
    "from DatasetLoader import collate_fn as cf\n",
    "import DatasetLoader as DL\n",
    "import Model as M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25daceff",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Apply the functions in Preprocessing.py to clean the midi dataset. There are multiple files that are currupted or duplicated. \n",
    "For this analysis we are also going to use only Midi file with a timestamp of 4/4, like in the reference paper. This filtering is done in CleaningData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3973b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting Duplicates:: 100%|██████████| 2201/2201 [00:00<00:00, 2227.41it/s]\n",
      "100%|██████████| 2201/2201 [06:27<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "DeleteDuplicates()\n",
    "CleaningData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82aec08",
   "metadata": {},
   "source": [
    "# Preprocessing data:\n",
    "\n",
    "Firstly we reconstruct the database, transforming all the polyphonic audios into monophonic, keeping the information about the tracks (instruments) in each of the midi file. It is done by keeping only the highest pitch from each polyphonic note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recreating Database: 100%|██████████| 2059/2059 [08:41<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#RecreateDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33b80b",
   "metadata": {},
   "source": [
    "The input of the model has to be a 128x16 matrix as in the paper. The following function clasify the midi tracks into instrumental classes:\n",
    "- Guitar  \n",
    "- Percussion\n",
    "- Organ  \n",
    "- Sound Effects \n",
    "- Bass  \n",
    "- Piano \n",
    "- Synth Lead \n",
    "- Chromatic Percussion \n",
    "- Synth Pad  \n",
    "- Percussive \n",
    "- Synth Effects\n",
    "- Ethnic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = PreProcessing(nDir = 2000)\n",
    "torch.save(Dataset, 'Dataset_Percussion1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2403bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bass  115561\n",
      "Piano  58756\n",
      "Guitar  185958\n",
      "Organ  44142\n",
      "Ensemble  62419\n",
      "Synth Lead  21766\n",
      "Reed  30971\n",
      "Sound Effects  8824\n",
      "Brass  33210\n",
      "Chromatic Percussion  29047\n",
      "Pipe  18956\n",
      "Percussion  6663\n",
      "Synth Pad  17083\n",
      "Strings  33716\n",
      "Percussive  6366\n",
      "Synth Effects  7385\n",
      "Ethnic  3040\n"
     ]
    }
   ],
   "source": [
    "#Dataset = torch.load('Dataset.pt', map_location='cpu')\n",
    "\n",
    "for key in Dataset.keys():\n",
    "   print(key, '', len(Dataset[key]))\n",
    "\n",
    "#Since we load it after, we free some space\n",
    "# del Dataset\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baf7636",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "Dataset['Bass']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb5535",
   "metadata": {},
   "source": [
    "# Monophonic Model and Architecture\n",
    "\n",
    "The class DatasetTransorm allow us to choose which intrument's bars to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09475244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are selecting the data from the dataset with the guitar instrument\n",
    "\n",
    "from DatasetLoader import collate_fn as cf\n",
    "\n",
    "Data = DL.MonophonicDataset(Instrument='Guitar')\n",
    "trainData = DataLoader(Data, batch_size=10, shuffle=True, num_workers=0, collate_fn=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d495b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56575f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4e93f5",
   "metadata": {},
   "source": [
    "# Genre detection using CNN\n",
    "\n",
    "The idea is to train a Convolutional Neural Network (CNN) to understand the structure of the songs and to implement a classifier capable of identifying the genre of each song in our MIDI dataset.\n",
    "\n",
    "However, we cannot train the CNN directly on our MIDI dataset, since this would compromise both learning and classification. Moreover, CNNs are supervised learning models, and our dataset does not include genre labels. For this reason, we found another dataset containing 100 songs in .wav format for each of the following musical genres:\n",
    "\n",
    "- metal\n",
    "- disco\n",
    "- classical\n",
    "- hiphop\n",
    "- jazz\n",
    "- country\n",
    "- pop\n",
    "- blues \n",
    "- raggae \n",
    "- rock\n",
    "\n",
    "The idea is to train the CNN using this labeled dataset. Before doing that, we need to perform some preprocessing, since some songs in the dataset are corrupted. Additionally, the audio clips are only a few seconds long, so we preprocess each song to have a fixed length and a consistent format. The preprocessing functions are implemented in the file **CNN_ExtractGenre.py**.\n",
    "\n",
    "After preprocessing, we define the CNN model and the data loader in **Model.py** and **DataLoader.py**, respectively. The model is trained on Google Colab (not on the local machine), and we later load the trained model using its state_dict.\n",
    "\n",
    "The CNN achieves a strong validation accuracy of 84%, as shown in the accompanying paper.\n",
    "\n",
    "Once the model is trained on the labeled dataset, we use it to classify our own songs. This is a complex process because our songs are in .mid format, while the model expects .wav spectrograms as input. Therefore, each MIDI file must be converted into audio, transformed into a spectrogram, and then classified by the CNN.\n",
    "\n",
    "After classifying each song, we save a file containing the song’s name and its predicted genre. From there, we proceed as before: we separate our dataset by genre, and within each genre, we further separate the songs by instrument.\n",
    "\n",
    "\n",
    "N.B. all the function in the file **CNN_ExtractGenre** has already been runned since the computation is quite long. In the following cell we are showing the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30a7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gordon Lightfoot/Sundown (9, 0.5)\n",
      "Gordon Lightfoot/Sundown.1 (9, 1.0)\n",
      "Gordon Lightfoot/Rainy Day People (9, 0.5)\n",
      "Gordon Lightfoot/Carefree Highway (9, 0.92)\n",
      "Gordon Lightfoot/Beautiful (4, 0.67)\n",
      "Gounod Charles/Ave Maria.1 (2, 0.92)\n",
      "Gounod Charles/Marche funebre d'une marionnette (2, 0.42)\n",
      "Gounod Charles/Waltz from Faust (2, 1.0)\n",
      "Grace Jones/Slave to the Rhythm (6, 0.58)\n",
      "Grand Funk Railroad/Some Kind of Wonderful (1, 1.0)\n",
      "Grand Funk Railroad/I'm Your Captain (Closer to Home) (9, 0.67)\n"
     ]
    }
   ],
   "source": [
    "#Load the preprocessed and classified dataset:\n",
    "with open('GenreDataset.pkl', 'rb') as f:\n",
    "   GenreDataset = pickle.load(f)\n",
    "\n",
    "#Mapping each genre into a number for classification\n",
    "GenreMapping = {'metal': 0, 'disco': 1, 'classical': 2, 'hiphop': 3, 'jazz': 4,\n",
    "          'country': 5, 'pop': 6, 'blues': 7, 'reggae': 8, 'rock': 9}\n",
    "\n",
    "i = 0\n",
    "for key in GenreDataset.keys():\n",
    "   i += 1\n",
    "   print(key, GenreDataset[key])\n",
    "\n",
    "   if i > 10:\n",
    "      break\n",
    "#Author/Name of the song, (Genre, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0e127",
   "metadata": {},
   "source": [
    "# Polyphonic Music Generator\n",
    "\n",
    "as now we are only considering monophonic tracks, we are thus losing all the information between notes of the same instrument and the correlation between instruments! Therefore we would like to try to implement a polyphonic music generator. \n",
    "\n",
    "The strategy is the same as before. Instead of having (128x16) matrix we have (4x128x16) where 4 is the maximum number of instruments that can be played at the same time. Now each matrix 128x16 does not encode for a single note as before, but it allows for multiple note of the same instrument.\n",
    "\n",
    "In the file **PolyphonicPreprocessing.py** there are all the function used to preprocess the clean_midi dataset and build the dataset of mapped songs, cathegorized by genre (using the genre recognition dataset built before). Here we are processing and storing the Polyphonic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolyphonicDataset = PolyphonicPreProcessing(nDir = 2000)\n",
    "torch.save(PolyphonicDataset, 'PolyphonicDataset_Percussion.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddd389",
   "metadata": {},
   "source": [
    "And these are all the genre and the number of bars for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b133b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disco  14828\n",
      "country  9742\n",
      "rock  62342\n",
      "jazz  9811\n",
      "classical  9943\n",
      "pop  8046\n",
      "blues  1513\n",
      "raggae  3262\n",
      "hiphop  228\n",
      "metal  88\n"
     ]
    }
   ],
   "source": [
    "PolyphonicDataset = torch.load('PolyphonicDataset.pt', weights_only=False)\n",
    "\n",
    "for key in PolyphonicDataset:\n",
    "   print(key, '', len(PolyphonicDataset[key]))\n",
    "\n",
    "#We load with the DataLoader class, freeing some space\n",
    "# del PolyphonicDataset\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba03369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 2,  2,  2],\n",
       "                       [56, 61, 63],\n",
       "                       [ 6,  4,  0]]),\n",
       "       values=tensor([1, 1, 1]),\n",
       "       size=(4, 128, 16), nnz=3, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolyphonicDataset['disco'][0]['Bars'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18696ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(PolyphonicDataset['disco'][0]['Bars'][0].to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc14a19",
   "metadata": {},
   "source": [
    "And we can do the same thing done before saving the velocity information of the bars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a951e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2000/2000 [07:19<00:00,  4.55it/s] \n",
      "100%|██████████| 10/10 [00:00<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "PolyphonicDatasetVelocity = PolyphonicPreProcessing(nDir = 2000, Velocity=True)\n",
    "torch.save(PolyphonicDatasetVelocity, 'PolyphonicDatasetVelocity.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3901c4",
   "metadata": {},
   "source": [
    "And we can load the dataset using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452518b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolData = DL.PolyphonicDataset(Genre = 'jazz')\n",
    "PolTrainData = DataLoader(PolData, batch_size=30, shuffle=True, num_workers=0, collate_fn=cf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
